{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/haausing/miniconda3/envs/evalplus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-12 21:30:57,181\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 21:30:57 llm_engine.py:72] Initializing an LLM engine with config: model='codellama/CodeLlama-7b-Python-hf', tokenizer='codellama/CodeLlama-7b-Python-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir='/mnt/scratch-artemis/haausing/hf_cache', load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
      "INFO 02-12 21:31:01 weight_utils.py:164] Using model weights format ['*.safetensors']\n",
      "INFO 02-12 21:31:05 llm_engine.py:322] # GPU blocks: 3763, # CPU blocks: 512\n",
      "INFO 02-12 21:31:07 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-12 21:31:07 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-12 21:31:13 model_runner.py:698] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "#os.environ['HF_HOME'] = '/mnt/scratch-artemis/haausing/hf_cache'\n",
    "\n",
    "nb = \"7b\"\n",
    "cache_dir = f\"/mnt/scratch-artemis/haausing/hf_cache\"\n",
    "model_name = f\"codellama/CodeLlama-{nb}-Python-hf\"\n",
    "llm = LLM(\n",
    "    model=model_name, \n",
    "    max_model_len=2048,\n",
    "    download_dir = cache_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import construct_contract_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalplus.data import get_mbpp_plus\n",
    "dataset = get_mbpp_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load prompt\n",
    "task_id = \"Mbpp/2\"\n",
    "task = dataset[task_id]\n",
    "contract_type = \"none\"\n",
    "prompt = construct_contract_prompt(task[\"prompt\"], contract_type, task[\"contract\"])\n",
    "#load generation\n",
    "output_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/mbpp/code-llama-7b_temp_0.8\"\n",
    "changed_task_id = task_id.replace(\"/\", \"_\")\n",
    "output_file = f\"{output_dir}/{changed_task_id}/0.py\"\n",
    "# load output_file as str\n",
    "with open(output_file, \"r\") as f:\n",
    "    generation = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_outputs = llm.generate(\n",
    "    [prompt, generation],\n",
    "    SamplingParams(\n",
    "        temperature=1,\n",
    "        max_tokens=1,\n",
    "        top_p=1.0,\n",
    "        prompt_logprobs=0\n",
    "    ),\n",
    "    use_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = vllm_outputs[0].prompt_token_ids\n",
    "assert vllm_outputs[1].prompt_token_ids[:len(prompt_ids)] == prompt_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = vllm_outputs[1].prompt_logprobs[len(prompt_ids):]# logprobs for the generation\n",
    "# extract values of each dict\n",
    "log_probs = [list(d.values())[0] for d in log_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3335464596748352,\n",
       " -0.5658361911773682,\n",
       " -0.09111369401216507,\n",
       " -0.11316433548927307,\n",
       " -0.0036429488100111485,\n",
       " -0.02000465989112854,\n",
       " -0.012292811647057533,\n",
       " -1.9872897863388062,\n",
       " -0.2220921516418457,\n",
       " -0.22801125049591064,\n",
       " -0.00937500037252903,\n",
       " -0.00028713393840007484,\n",
       " -0.05111154913902283,\n",
       " -0.0031545422971248627,\n",
       " -0.09514015167951584,\n",
       " -1.0171672105789185,\n",
       " -0.3694011867046356,\n",
       " -0.027140766382217407,\n",
       " -0.03143935278058052,\n",
       " -0.006101434119045734,\n",
       " -0.25642383098602295,\n",
       " -0.0021571479737758636,\n",
       " -0.0011356578906998038,\n",
       " -0.0007798014557920396,\n",
       " -0.6346943974494934,\n",
       " -0.0002215855201939121,\n",
       " -0.0001867835089797154,\n",
       " -0.0013525871327146888,\n",
       " -0.0005881248507648706,\n",
       " -0.03968319296836853]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

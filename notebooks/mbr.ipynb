{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from warnings import warn\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from evalplus.data.utils import CACHE_DIR\n",
    "\n",
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "\n",
    "from evalplus.eval._special_oracle import (\n",
    "    MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    MBPP_OUTPUT_SET_EQ_TASKS,\n",
    "    _poly,\n",
    ")\n",
    "\n",
    "from evalplus.gen.util import trusted_exec\n",
    "\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False\n",
    "\n",
    "def ut_exact_match(\n",
    "    hyp_ut, \n",
    "    ref_ut, \n",
    "    entry_point, \n",
    "    dataset, \n",
    "    inp=None, \n",
    "    atol=0 # need to change this later\n",
    "    ):\n",
    "    exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    # ================================================ #\n",
    "    # ============== special oracles ================= #\n",
    "    if dataset == \"mbpp\":\n",
    "        if \"are_equivalent\" == entry_point:  # Mbpp/164 special oracle\n",
    "            exact_match = exact_match or True\n",
    "        elif \"sum_div\" == entry_point:  # Mbpp/295 special oracle\n",
    "            exact_match = exact_match or hyp_ut == 0 or ref_ut == 0\n",
    "        elif entry_point in MBPP_OUTPUT_SET_EQ_TASKS:\n",
    "            exact_match = set(hyp_ut) == set(ref_ut)\n",
    "        elif entry_point in MBPP_OUTPUT_NOT_NONE_TASKS:\n",
    "            # exp is True  if not None\n",
    "            #        False if None\n",
    "            if isinstance(hyp_ut, bool):\n",
    "                hyp_ut = hyp_ut is not None\n",
    "            if isinstance(ref_ut, bool):\n",
    "                ref_ut = ref_ut is not None\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    if dataset == \"humaneval\":\n",
    "        if \"find_zero\" == entry_point:\n",
    "            hyp_ut = _poly(*inp, hyp_ut) <= atol\n",
    "            ref_ut = _poly(*inp, ref_ut) <= atol\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "    # ============== special oracles ================= #\n",
    "    # ================================================ #\n",
    "\n",
    "    if atol == 0 and (is_floats(ref_ut) or is_floats(hyp_ut)):\n",
    "        atol = 1e-6  # enforce atol for float comparison\n",
    "    if not exact_match and atol != 0:\n",
    "        # explicitly set rtol=1e-07\n",
    "        # to match `np.testing.assert_allclose`'s default values\n",
    "        exact_match =  np.allclose(hyp_ut, ref_ut, rtol=1e-07, atol=atol)\n",
    "    \n",
    "    return int(exact_match)\n",
    "\n",
    "def get_groundtruth(problems, hashcode, tasks_only_output_not_none):\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{hashcode}.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        #print(f\"Load from ground-truth from {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    #print(\"Computing expected output...\")\n",
    "    tbegin = time.time()\n",
    "    expected_output = {}\n",
    "    for task_id, problem in problems.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"base_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"plus_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "        expected_output[task_id] = oracle\n",
    "    #print(f\"Expected outputs computed in {time.time() - tbegin:.2f}s\")\n",
    "\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(expected_output, f)\n",
    "\n",
    "    return expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_exec(hyp_uts, ref_uts, entry_point, dataset, n_uts, inps=None, granular=False):\n",
    "    n_matches = 0\n",
    "    for i in range(n_uts):\n",
    "        # skip if either hyp_ut or ref_ut is not in the list\n",
    "        if i not in hyp_uts or i not in ref_uts:\n",
    "            continue\n",
    "        # if there's an error, we return 0\n",
    "        if type(hyp_uts[i]) == str and hyp_uts[i].startswith(\"failed:\"):\n",
    "            return 0 \n",
    "        if type(ref_uts[i]) == str and ref_uts[i].startswith(\"failed:\"):\n",
    "            return 0\n",
    "        # we start counting the number of matches\n",
    "        try:\n",
    "            n_matches += ut_exact_match(\n",
    "                hyp_uts[i], \n",
    "                ref_uts[i], \n",
    "                entry_point, \n",
    "                dataset, \n",
    "                inp=inps[i] if inps else None\n",
    "                )\n",
    "        except:\n",
    "            n_matches += 0\n",
    "        \n",
    "    if granular:\n",
    "        try:\n",
    "            return n_matches/ n_uts\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        return int(n_matches == n_uts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec_outputs loaded\n",
      "exec_outputs_debug loaded\n",
      "eval_results loaded\n",
      "ape_eval_results loaded\n",
      "ape_3times_eval_results loaded\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs\"\n",
    "dataset = \"humaneval\"\n",
    "#gen_dir = \"deepseek-coder-33b-instruct_temp_0.8\"\n",
    "#gen_dir = \"deepseek-coder-7b-instruct-v1.5_temp_1.2\"\n",
    "gen_dir = \"deepseek-coder-6.7b-instruct_temp_1.2\"\n",
    "#gen_dir = \"code-llama-13b-instruct_temp_1.6\"\n",
    "#gen_dir = \"code-llama-7b-instruct_temp_1.6\"\n",
    "#debug_gen_dir = gen_dir + \"_debug1_not_change_positive\"\n",
    "debug_gen_dir = gen_dir + \"_debug1_sd-ut\"\n",
    "debug_3times_gen_dir = gen_dir + \"_debug3_sd-ut\"\n",
    "#_debug1_not_change_positive\n",
    "# load exec_outputs\n",
    "\n",
    "# load problems\n",
    "if dataset == \"mbpp\":\n",
    "    problems = get_mbpp_plus()\n",
    "    dataset_hash = get_mbpp_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    )\n",
    "elif dataset == \"humaneval\":\n",
    "    problems = get_human_eval_plus()\n",
    "    dataset_hash = get_human_eval_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        []\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n",
    "\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/exec_outputs_v2.pkl\", \"rb\") as f:\n",
    "    exec_outputs = pickle.load(f)\n",
    "print(\"exec_outputs loaded\")\n",
    "\n",
    "# load exec_outputs\n",
    "with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/exec_outputs_v2.pkl\", \"rb\") as f:\n",
    "    exec_outputs_debug = pickle.load(f)\n",
    "print(\"exec_outputs_debug loaded\")\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "    \n",
    "#pop out [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]\n",
    "if dataset == \"mbpp\":\n",
    "    for task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        eval_results[\"eval\"].pop(task_id)\n",
    "print(\"eval_results loaded\")\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    ape_eval_results = json.load(f)\n",
    "for task_id in ape_eval_results[\"eval\"]:\n",
    "    ape_eval_results[\"eval\"][task_id] = sorted(ape_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "print(\"ape_eval_results loaded\")\n",
    "\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{debug_3times_gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    ape_3times_eval_results = json.load(f)\n",
    "for task_id in ape_3times_eval_results[\"eval\"]:\n",
    "    ape_3times_eval_results[\"eval\"][task_id] = sorted(ape_3times_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "print(\"ape_3times_eval_results loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"humaneval\":\n",
    "    with open(f\"{work_dir}/{dataset}/{gen_dir}/errors.pkl\", \"rb\") as f:\n",
    "        errors = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_task(task_id, eval_results_dict, exec_outputs_dict, max_hyps = 200, start_id = 0, num_base_test_cases = 3,num_plus_test_cases=3, granular=False, filter = True):\n",
    "    test_all_plus_cases = True\n",
    "    test_all_base_cases = True\n",
    "    assert start_id >= 0 and start_id + max_hyps <= 200\n",
    "    n_expected_outputs_base = len(expected_output[task_id][\"base\"])\n",
    "    n_expected_outputs_plus = len(expected_output[task_id][\"plus\"])\n",
    "    if num_base_test_cases < n_expected_outputs_base:\n",
    "        test_all_base_cases = False\n",
    "    if num_plus_test_cases < n_expected_outputs_plus:\n",
    "        test_all_plus_cases = False\n",
    "    task_utility_base = []\n",
    "    task_utility_plus = []\n",
    "    \n",
    "    for hyp_id, hyp in enumerate(eval_results_dict[\"eval\"][task_id]):\n",
    "        if hyp_id >= max_hyps + start_id or hyp_id < start_id:\n",
    "            continue\n",
    "        hyp_base_outputs = exec_outputs_dict[task_id][hyp_id][\"base\"]\n",
    "        hyp_plus_outputs = exec_outputs_dict[task_id][hyp_id][\"plus\"]\n",
    "        if not test_all_base_cases:\n",
    "            hyp_base_outputs = {i: hyp_base_outputs[i] for i in range(num_base_test_cases) if i < min(num_base_test_cases, len(hyp_base_outputs))}\n",
    "        if not test_all_plus_cases:\n",
    "            hyp_plus_outputs = {i: hyp_plus_outputs[i] for i in range(num_plus_test_cases) if i < min(num_plus_test_cases, len(hyp_plus_outputs))}\n",
    "        hyp_utility_base = []\n",
    "        hyp_utility_plus = []\n",
    "        \n",
    "        for ref_id, ref in enumerate(eval_results_dict[\"eval\"][task_id]):\n",
    "            if ref_id >= max_hyps + start_id or ref_id < start_id:\n",
    "                continue\n",
    "            if ref[\"base_status\"] == ref[\"plus_status\"] == hyp[\"base_status\"] == hyp[\"plus_status\"] == \"pass\":\n",
    "                hyp_utility_base.append(1)\n",
    "                hyp_utility_plus.append(1)\n",
    "                continue\n",
    "            if filter:\n",
    "                ### add the filtering baseline\n",
    "                if dataset == \"humaneval\":\n",
    "                    if errors[task_id][hyp_id][\"base\"][\"status\"] != \"pass\":\n",
    "                        hyp_utility_base.append(0)\n",
    "                        hyp_utility_plus.append(0)\n",
    "                        continue\n",
    "                elif dataset == \"mbpp\":\n",
    "                    if len(hyp[\"base_details\"]) == 0 or hyp[\"base_details\"][0] == 0:\n",
    "                        hyp_utility_base.append(0)\n",
    "                        hyp_utility_plus.append(0)\n",
    "                        continue\n",
    "                ### end of filtering baseline\n",
    "            ref_base_outputs = exec_outputs_dict[task_id][ref_id][\"base\"]\n",
    "            ref_plus_outputs = exec_outputs_dict[task_id][ref_id][\"plus\"]\n",
    "            if not test_all_base_cases:\n",
    "                ref_base_outputs = {i: ref_base_outputs[i] for i in range(num_base_test_cases) if i < min(num_base_test_cases, len(ref_base_outputs))}\n",
    "            if not test_all_plus_cases:\n",
    "                ref_plus_outputs = {i: ref_plus_outputs[i] for i in range(num_plus_test_cases) if i < min(num_plus_test_cases, len(ref_plus_outputs))}\n",
    "            \n",
    "            util_score_base = mbr_exec(hyp_base_outputs, ref_base_outputs, problems[task_id][\"entry_point\"], \"mbpp\", n_expected_outputs_base, granular=granular)\n",
    "            util_score_plus = mbr_exec(hyp_plus_outputs, ref_plus_outputs, problems[task_id][\"entry_point\"], \"mbpp\", n_expected_outputs_plus, granular=granular)\n",
    "            hyp_utility_base.append(util_score_base)\n",
    "            if granular:\n",
    "                hyp_utility_plus.append((util_score_plus*n_expected_outputs_plus+util_score_base*n_expected_outputs_base)/(n_expected_outputs_plus+n_expected_outputs_base))\n",
    "            else:\n",
    "                if num_plus_test_cases == 0:\n",
    "                    hyp_utility_plus.append(util_score_base)\n",
    "                else:\n",
    "                    hyp_utility_plus.append(int(util_score_plus==util_score_base==1))\n",
    "            #hyp_utility_plus.append(mbr_exec(hyp_plus_outputs, ref_plus_outputs, problems[task_id][\"entry_point\"], \"mbpp\", n_expected_outputs_plus))\n",
    "        #hyp_utility_plus.extend(hyp_utility_base)\n",
    "        task_utility_base.append(np.mean(hyp_utility_base))\n",
    "        task_utility_plus.append(np.mean(hyp_utility_plus))\n",
    "    \n",
    "    # get argmax\n",
    "    argmax_base = np.argmax(task_utility_base) + start_id\n",
    "    argmax_plus = np.argmax(task_utility_plus) + start_id\n",
    "    if not test_all_base_cases:\n",
    "        argmax_plus = argmax_base\n",
    "    assert argmax_base == int(eval_results_dict[\"eval\"][task_id][argmax_base][\"solution_id\"])\n",
    "    assert argmax_plus == int(eval_results_dict[\"eval\"][task_id][argmax_plus][\"solution_id\"])\n",
    "    base_status = eval_results_dict[\"eval\"][task_id][argmax_base][\"base_status\"]\n",
    "    plus_status = eval_results_dict[\"eval\"][task_id][argmax_plus][\"plus_status\"]\n",
    "    return (int(base_status == \"pass\"), int(base_status == plus_status == \"pass\"), argmax_base, argmax_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(eval_results, exec_outputs, max_hyps=200, start_id=0, num_base_test_cases=10, num_plus_test_cases=300, granular=False, filter=True, workers=20):\n",
    "    for task_id in eval_results[\"eval\"]:\n",
    "        eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "        \n",
    "    base_results = {}\n",
    "    plus_results = {}\n",
    "    argmax_bases = {}\n",
    "    argmax_pluss = {}\n",
    "    \n",
    "    def process_single_task(task_id):\n",
    "        base_result, plus_result, argmax_base, argmax_plus = process_task(task_id, \n",
    "                                                                          eval_results, \n",
    "                                                                          exec_outputs, \n",
    "                                                                          max_hyps=max_hyps, \n",
    "                                                                          start_id=start_id, \n",
    "                                                                          num_base_test_cases=num_base_test_cases,\n",
    "                                                                          num_plus_test_cases=num_plus_test_cases, \n",
    "                                                                          granular=granular, \n",
    "                                                                          filter=filter)\n",
    "        base_results[task_id] = base_result\n",
    "        plus_results[task_id] = plus_result\n",
    "        argmax_bases[task_id] = argmax_base\n",
    "        argmax_pluss[task_id] = argmax_plus\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        list(tqdm(executor.map(process_single_task, eval_results[\"eval\"]), total=len(eval_results[\"eval\"])))\n",
    "        \n",
    "    for task_id in ape_3times_eval_results[\"eval\"]:\n",
    "        ape_3times_eval_results[\"eval\"][task_id] = sorted(ape_3times_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "\n",
    "    ape_base_results = {}\n",
    "    ape_plus_results = {}\n",
    "    for task_id in ape_3times_eval_results[\"eval\"]:\n",
    "        argmax_base_solution = ape_3times_eval_results[\"eval\"][task_id][argmax_bases[task_id]]\n",
    "        argmax_plus_solution = ape_3times_eval_results[\"eval\"][task_id][argmax_pluss[task_id]]\n",
    "        argmax_base_status = argmax_base_solution[\"base_status\"]\n",
    "        argmax_plus_status = argmax_plus_solution[\"plus_status\"]\n",
    "        assert int(argmax_base_solution[\"solution_id\"]) == argmax_bases[task_id]\n",
    "        assert int(argmax_plus_solution[\"solution_id\"]) == argmax_pluss[task_id]\n",
    "        ape_base_results[task_id] = int(argmax_base_status == \"pass\")\n",
    "        ape_plus_results[task_id] = int(argmax_plus_status == argmax_base_status == \"pass\")\n",
    "    \n",
    "    return base_results, plus_results, ape_base_results, ape_plus_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_debugged_results(ape_eval_results, exec_outputs, max_hyps=200, start_id=0, num_base_test_cases=10, num_plus_test_cases=300, granular=False, filter=True, workers=20):\n",
    "    for task_id in ape_eval_results[\"eval\"]:\n",
    "        ape_eval_results[\"eval\"][task_id] = sorted(ape_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "    \n",
    "    debugged_base_results = {}\n",
    "    debugged_plus_results = {}\n",
    "    debugged_argmax_bases = {}\n",
    "    debugged_argmax_pluss = {}\n",
    "\n",
    "    def process_single_task(task_id):\n",
    "        debugged_base_result, debugged_plus_result, debugged_argmax_base, debugged_argmax_plus = process_task(task_id, \n",
    "                                                                                                              ape_eval_results, \n",
    "                                                                                                              exec_outputs,\n",
    "                                                                                                              max_hyps=max_hyps, \n",
    "                                                                                                              start_id=start_id, \n",
    "                                                                                                              num_base_test_cases=num_base_test_cases,\n",
    "                                                                                                              num_plus_test_cases=num_plus_test_cases,\n",
    "                                                                                                              granular=granular,\n",
    "                                                                                                              filter=filter)\n",
    "        debugged_base_results[task_id] = debugged_base_result\n",
    "        debugged_plus_results[task_id] = debugged_plus_result\n",
    "        debugged_argmax_bases[task_id] = debugged_argmax_base\n",
    "        debugged_argmax_pluss[task_id] = debugged_argmax_plus\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        list(tqdm(executor.map(process_single_task, ape_eval_results[\"eval\"]), total=len(ape_eval_results[\"eval\"])))\n",
    "\n",
    "    return debugged_base_results, debugged_plus_results, debugged_argmax_bases, debugged_argmax_pluss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 116/164 [00:02<00:00, 50.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:03<00:00, 51.53it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 41.61it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 43.94it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 51.11it/s]\n"
     ]
    }
   ],
   "source": [
    "num_base_test_cases = 300\n",
    "num_plus_test_cases = 10\n",
    "max_hyps = 50\n",
    "granular = False\n",
    "filter = True\n",
    "workers = 1\n",
    "base_score = []\n",
    "plus_score = []\n",
    "ape_base_score = []\n",
    "ape_plus_score = []\n",
    "debugged_base_score = []\n",
    "debugged_plus_score = []\n",
    "for start_id in range(0, 200, max_hyps):\n",
    "    base_results, plus_results, ape_base_results, ape_plus_results = get_results(eval_results,\n",
    "                                                                                 exec_outputs, \n",
    "                                                                                 max_hyps=max_hyps, \n",
    "                                                                                 start_id=start_id, \n",
    "                                                                                 num_base_test_cases=num_base_test_cases,\n",
    "                                                                                 num_plus_test_cases=num_plus_test_cases,\n",
    "                                                                                 granular=granular,\n",
    "                                                                                 filter=filter,\n",
    "                                                                                 workers=workers)\n",
    "    #debugged_base_results, debugged_plus_results, debugged_argmax_bases, debugged_argmax_pluss = get_debugged_results(ape_eval_results, \n",
    "    #                                                                                                                      exec_outputs_debug, \n",
    "    #                                                                                                                      max_hyps=max_hyps, \n",
    "    #                                                                                                                      start_id=start_id, \n",
    "    #                                                                                                                      num_base_test_cases=num_base_test_cases,\n",
    "    #                                                                                                                      num_plus_test_cases=num_plus_test_cases,\n",
    "    #                                                                                                                      granular=granular,\n",
    "    #                                                                                                                      filter=filter,\n",
    "    #                                                                                                                      workers=workers)\n",
    "    \n",
    "    base_score.append(sum(base_results.values())/len(base_results))\n",
    "    plus_score.append(sum(plus_results.values())/len(plus_results))\n",
    "    ape_base_score.append(sum(ape_base_results.values())/len(ape_base_results))\n",
    "    ape_plus_score.append(sum(ape_plus_results.values())/len(ape_plus_results))\n",
    "    #debugged_base_score.append(sum(debugged_base_results.values())/len(debugged_base_results))\n",
    "    #debugged_plus_score.append(sum(debugged_plus_results.values())/len(debugged_plus_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: code-llama-13b-instruct_temp_1.6\n",
      "dataset: humaneval\n",
      "filter: True\n",
      "number of Test Cases: 20\n",
      "number of hypotheses: 50\n",
      "MBR base      80.487804878 %\n",
      "MBR->APE base 80.9451219512 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MBR plus      76.0670731707 %\n",
      "MBR->APE plus 76.5243902439 %\n"
     ]
    }
   ],
   "source": [
    "round_digits = 10\n",
    "num_base_tests = 10\n",
    "print(\"model: {}\".format(gen_dir))\n",
    "print(\"dataset: {}\".format(dataset))\n",
    "print(\"filter: {}\".format(filter))\n",
    "print(\"number of Test Cases: {}\".format(num_plus_test_cases+num_base_tests))\n",
    "print(\"number of hypotheses: {}\".format(max_hyps))\n",
    "print(\"MBR base     \", np.round(sum(base_score)/len(base_score) * 100, round_digits), \"%\")\n",
    "print(\"MBR->APE base\", np.round(sum(ape_base_score)/len(ape_base_score) * 100, round_digits), \"%\")\n",
    "#print(\"APE->MBR base\", np.round(sum(debugged_base_score)/len(debugged_base_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"MBR plus     \", np.round(sum(plus_score)/len(plus_score) * 100, round_digits), \"%\")\n",
    "print(\"MBR->APE plus\", np.round(sum(ape_plus_score)/len(ape_plus_score) * 100, round_digits), \"%\")\n",
    "#print(\"APE->MBR plus\", np.round(sum(debugged_plus_score)/len(debugged_plus_score) * 100, round_digits), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8405063291139241, 0.8481012658227848, 0.8430379746835444, 0.830379746835443]\n",
      "[0.7518987341772152, 0.7518987341772152, 0.7518987341772152, 0.7341772151898734]\n",
      "[0.8430379746835444, 0.8481012658227848, 0.8405063291139241, 0.8253164556962025]\n",
      "[0.7493670886075949, 0.7468354430379747, 0.7468354430379747, 0.7265822784810126]\n",
      "[0.8405063291139241, 0.850632911392405, 0.8582278481012658, 0.8481012658227848]\n",
      "[0.7569620253164557, 0.7645569620253164, 0.7670886075949367, 0.759493670886076]\n"
     ]
    }
   ],
   "source": [
    "print(base_score)\n",
    "print(plus_score)\n",
    "print(ape_base_score)\n",
    "print(ape_plus_score)\n",
    "print(debugged_base_score)\n",
    "print(debugged_plus_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: code-llama-7b-instruct_temp_1.6\n",
    "# dataset: mbpp\n",
    "# filter: True\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      78.9873417722 %\n",
    "# MBR->APE base 79.3037974684 %\n",
    "# APE->MBR base 81.2658227848 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      69.3037974684 %\n",
    "# MBR->APE plus 69.1772151899 %\n",
    "# APE->MBR plus 72.0886075949 %\n",
    "\n",
    "# model: code-llama-7b-instruct_temp_1.6\n",
    "# dataset: mbpp\n",
    "# filter: False\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      63.9873417722 %\n",
    "# MBR->APE base 65.7594936709 %\n",
    "# APE->MBR base 66.3291139241 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      57.8481012658 %\n",
    "# MBR->APE plus 59.0506329114 %\n",
    "# APE->MBR plus 60.3797468354 %\n",
    "\n",
    "# model: code-llama-7b-instruct_temp_1.6\n",
    "# dataset: humaneval\n",
    "# filter: True\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      74.5426829268 %\n",
    "# MBR->APE base 74.3902439024 %\n",
    "# APE->MBR base 76.5243902439 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      70.1219512195 %\n",
    "# MBR->APE plus 70.1219512195 %\n",
    "# APE->MBR plus 72.256097561 %\n",
    "\n",
    "# model: code-llama-7b-instruct_temp_1.6\n",
    "# dataset: humaneval\n",
    "# filter: False\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      52.1341463415 %\n",
    "# MBR->APE base 51.9817073171 %\n",
    "# APE->MBR base 52.4390243902 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      49.5426829268 %\n",
    "# MBR->APE plus 49.5426829268 %\n",
    "# APE->MBR plus 50.0 %\n",
    "\n",
    "# model: deepseek-coder-6.7b-instruct_temp_1.2\n",
    "# dataset: mbpp\n",
    "# filter: True\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      89.1139240506 %\n",
    "# MBR->APE base 88.9873417722 %\n",
    "# APE->MBR base 89.8734177215 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      82.0886075949 %\n",
    "# MBR->APE plus 81.7088607595 %\n",
    "# APE->MBR plus 82.8481012658 %\n",
    "\n",
    "# model: deepseek-coder-6.7b-instruct_temp_1.2\n",
    "# dataset: mbpp\n",
    "# filter: False\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      83.9873417722 %\n",
    "# MBR->APE base 84.5569620253 %\n",
    "# APE->MBR base 84.8734177215 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      78.6075949367 %\n",
    "# MBR->APE plus 78.7974683544 %\n",
    "# APE->MBR plus 79.3037974684 %\n",
    "\n",
    "# model: deepseek-coder-6.7b-instruct_temp_1.2\n",
    "# dataset: humaneval\n",
    "# filter: True\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      91.6158536585 %\n",
    "# MBR->APE base 91.4634146341 %\n",
    "# APE->MBR base 91.6158536585 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      90.7012195122 %\n",
    "# MBR->APE plus 90.3963414634 %\n",
    "# APE->MBR plus 90.3963414634 %\n",
    "\n",
    "# model: deepseek-coder-6.7b-instruct_temp_1.2\n",
    "# dataset: humaneval\n",
    "# filter: False\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      86.4329268293 %\n",
    "# MBR->APE base 87.0426829268 %\n",
    "# APE->MBR base 87.6524390244 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      85.5182926829 %\n",
    "# MBR->APE plus 85.9756097561 %\n",
    "# APE->MBR plus 87.0426829268 %\n",
    "\n",
    "# model: code-llama-13b-instruct_temp_1.6\n",
    "# dataset: mbpp\n",
    "# filter: True\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      84.0506329114 %\n",
    "# MBR->APE base 83.9240506329 %\n",
    "# APE->MBR base 84.9367088608 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      74.746835443 %\n",
    "# MBR->APE plus 74.2405063291 %\n",
    "# APE->MBR plus 76.2025316456 %\n",
    "\n",
    "# model: code-llama-13b-instruct_temp_1.6\n",
    "# dataset: mbpp\n",
    "# filter: False\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      73.9240506329 %\n",
    "# MBR->APE base 74.3670886076 %\n",
    "# APE->MBR base 74.4303797468 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      67.2151898734 %\n",
    "# MBR->APE plus 66.9620253165 %\n",
    "# APE->MBR plus 68.3544303797 %\n",
    "\n",
    "# model: code-llama-13b-instruct_temp_1.6\n",
    "# dataset: humaneval\n",
    "# filter: True\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      80.487804878 %\n",
    "# MBR->APE base 80.9451219512 %\n",
    "# APE->MBR base 81.5548780488 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      76.0670731707 %\n",
    "# MBR->APE plus 76.5243902439 %\n",
    "# APE->MBR plus 77.5914634146 %\n",
    "\n",
    "# model: code-llama-13b-instruct_temp_1.6\n",
    "# dataset: humaneval\n",
    "# filter: False\n",
    "# number of Test Cases: 503\n",
    "# number of hypotheses: 50\n",
    "# MBR base      61.2804878049 %\n",
    "# MBR->APE base 61.737804878 %\n",
    "# APE->MBR base 62.6524390244 %\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# MBR plus      59.6036585366 %\n",
    "# MBR->APE plus 60.0609756098 %\n",
    "# APE->MBR plus 61.4329268293 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_majority_votes = {}\n",
    "for task in exec_outputs:\n",
    "    if dataset == \"mbpp\":\n",
    "        if task in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "            continue\n",
    "    new_majority_votes[task] = []\n",
    "    for i in range(len(exec_outputs[task])):\n",
    "        try:\n",
    "            new_majority_votes[task].append(exec_outputs[task][i][\"base\"][0])\n",
    "        except:\n",
    "            new_majority_votes[task].append(\"failed: couldn't collect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = {}\n",
    "for task_id in new_majority_votes:\n",
    "    all_ids[task_id] = []\n",
    "    candidate_set = []\n",
    "    candidates = {i: e for i, e in enumerate(new_majority_votes[task_id]) if not str(e).startswith(\"failed:\")}\n",
    "    if len(candidates) == 0:\n",
    "        all_ids[task_id] = [[]]\n",
    "        continue\n",
    "    for id in candidates:\n",
    "        if candidates[id] not in candidate_set:\n",
    "            candidate_set.append(candidates[id])\n",
    "            all_ids[task_id].append([id])\n",
    "        else:\n",
    "            index = candidate_set.index(candidates[id])\n",
    "            all_ids[task_id][index].append(id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lists_dict = {}\n",
    "for task_id in all_ids:\n",
    "    lengths = [len(e) for e in all_ids[task_id]]\n",
    "    max_length = max(lengths)\n",
    "    final_list = []\n",
    "    for e in all_ids[task_id]:\n",
    "        if len(e) == max_length:\n",
    "            final_list = e\n",
    "    assert type(final_list) == list\n",
    "    final_lists_dict[task_id] = final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_correct = {}\n",
    "plus_correct = {}\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    base_correct[task_id] = {}\n",
    "    plus_correct[task_id] = {}\n",
    "    if task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        continue\n",
    "    for i in final_lists_dict[task_id]:\n",
    "        assert i == int(eval_results[\"eval\"][task_id][i][\"solution_id\"])\n",
    "        base_correct[task_id][i] = eval_results[\"eval\"][task_id][i][\"base_status\"] == \"pass\"\n",
    "        plus_correct[task_id][i] = eval_results[\"eval\"][task_id][i][\"base_status\"] == eval_results[\"eval\"][task_id][i][\"plus_status\"] == \"pass\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/errors.pkl\", \"rb\") as f:\n",
    "    errors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5494685796998906\n",
      "0.47226502174549423\n"
     ]
    }
   ],
   "source": [
    "base_scores = []\n",
    "plus_scores = []\n",
    "for min_, max_ in [(0,50), (50, 100), (100, 150), (150,200)]:\n",
    "    for task_id in base_correct:\n",
    "        base_correct_task = []\n",
    "        plus_correct_task = []\n",
    "        for i in range(min_, max_):\n",
    "            if i in base_correct[task_id]:\n",
    "                # if you want to calculate one with only one unit test without filtering, just comment the next lin\n",
    "                #if errors[task_id][i][\"base\"][\"status\"] == \"pass\":\n",
    "                base_correct_task.append(base_correct[task_id][i])\n",
    "            if i in plus_correct[task_id]:\n",
    "                #if errors[task_id][i][\"base\"][\"status\"] == \"pass\":\n",
    "                plus_correct_task.append(plus_correct[task_id][i])\n",
    "        try:\n",
    "            base_scores.append(sum(base_correct_task)/len(base_correct_task))\n",
    "        except:\n",
    "            base_scores.append(0)\n",
    "        try:\n",
    "            plus_scores.append(sum(plus_correct_task)/len(plus_correct_task))\n",
    "        except:\n",
    "            plus_scores.append(0)\n",
    "\n",
    "print(np.mean(base_scores))\n",
    "print(np.mean(plus_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_vote(new_majority_votes, task_id):\n",
    "    counter = list(sorted(Counter(new_majority_votes[task_id]), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    counter = [e[0] for e in counter]\n",
    "    counter = [e for e in counter if not str(e).startswith(\"failed:\")]\n",
    "    if len(counter) != 0:\n",
    "        top_vote = counter[0]\n",
    "    else:\n",
    "        top_vote = \"failed: couldn't collect\"\n",
    "    collect_ids = []\n",
    "    for i, e in enumerate(new_majority_votes[task_id]):\n",
    "        if e == top_vote:\n",
    "            collect_ids.append(i)\n",
    "    return collect_ids\n",
    "\n",
    "total_base_correct = {}\n",
    "total_plus_correct = {}\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    collect_ids = get_top_vote(new_majority_votes, task_id)\n",
    "    base_correct = {}\n",
    "    plus_correct = {}\n",
    "    for i in collect_ids:\n",
    "        assert i == int(eval_results[\"eval\"][task_id][i][\"solution_id\"])\n",
    "        base_correct[i] = eval_results[\"eval\"][task_id][i][\"base_status\"] == \"pass\"\n",
    "        plus_correct[i] = eval_results[\"eval\"][task_id][i][\"base_status\"] == eval_results[\"eval\"][task_id][i][\"plus_status\"] == \"pass\"\n",
    "    total_base_correct[task_id] = base_correct\n",
    "    total_plus_correct[task_id] = plus_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9743108450645585\n",
      "0.9971335062771679\n",
      "0.9982035514862486\n",
      "0.9992159249137759\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# calculate the correlation using two lists\n",
    "print(np.corrcoef(mbr_base_scores, base_oracle)[1, 0])\n",
    "print(np.corrcoef(mbr_plus_scores, plus_oracle)[1, 0])\n",
    "print(np.corrcoef(filtering_mbr_base_scores, base_oracle)[1, 0])\n",
    "print(np.corrcoef(filtering_mbr_plus_scores, plus_oracle)[1, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cbs_f1 and cbs_f3 from pickle files\n",
    "import json\n",
    "import pickle\n",
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs\"\n",
    "dataset = \"mbpp\"\n",
    "#gen_dir = \"deepseek-coder-33b-instruct_temp_0.8\"\n",
    "#gen_dir = \"deepseek-coder-7b-instruct-v1.5_temp_0.8\"\n",
    "#gen_dir = \"deepseek-coder-6.7b-instruct_temp_1.2\"\n",
    "gen_dir = \"code-llama-13b-instruct_temp_1.6\"\n",
    "#gen_dir = \"code-llama-7b-instruct_temp_1.6\"\n",
    "#debug_gen_dir = gen_dir + \"_debug1_not_change_positive\"\n",
    "debug_gen_dir = gen_dir + \"_debug1_sd-ut\"\n",
    "#_debug1_not_change_positive\n",
    "# load exec_outputs§\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/cbs_f1.pkl\", \"rb\") as f:\n",
    "    cbs_f1 = pickle.load(f)\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/cbs_f3.pkl\", \"rb\") as f:\n",
    "    cbs_f3 = pickle.load(f)\n",
    "    \n",
    "#pop out [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]\n",
    "if dataset == \"mbpp\":\n",
    "    for task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        eval_results[\"eval\"].pop(task_id)\n",
    "        cbs_f1.pop(task_id)\n",
    "        cbs_f3.pop(task_id)\n",
    "    \n",
    "# load ape_eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    ape_eval_results = json.load(f)\n",
    "for task_id in ape_eval_results[\"eval\"]:\n",
    "    ape_eval_results[\"eval\"][task_id] = sorted(ape_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "\n",
    "#with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/cbs_f1.pkl\", \"rb\") as f:\n",
    "#    ape_cbs_f1 = pickle.load(f)\n",
    "#with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/cbs_f3.pkl\", \"rb\") as f:\n",
    "#    ape_cbs_f3 = pickle.load(f)\n",
    "\n",
    "#pop out [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]\n",
    "#for task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "#    ape_cbs_f1.pop(task_id)\n",
    "#    ape_cbs_f3.pop(task_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"humaneval\":\n",
    "    with open(f\"{work_dir}/{dataset}/{gen_dir}/errors.pkl\", \"rb\") as f:\n",
    "        errors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_task_mbr(task_id, eval_results_dict, utility_f1, utility_f3, max_hyps = 200, start_id = 0, filter = True):\n",
    "    assert start_id >= 0 and start_id + max_hyps <= len(eval_results_dict[\"eval\"][task_id])\n",
    "    task_utility_f1 = []\n",
    "    task_utility_f3 = []\n",
    "    \n",
    "    for hyp_id, hyp in enumerate(eval_results_dict[\"eval\"][task_id]):\n",
    "        if hyp_id >= max_hyps + start_id or hyp_id < start_id:\n",
    "            continue\n",
    "        hyp_utility_f1 = []\n",
    "        hyp_utility_f3 = []\n",
    "        \n",
    "        for ref_id, ref in enumerate(eval_results_dict[\"eval\"][task_id]):\n",
    "            if ref_id >= max_hyps + start_id or ref_id < start_id:\n",
    "                continue\n",
    "            if filter:\n",
    "                if dataset == \"mbpp\":\n",
    "                    ### add the filtering baseline\n",
    "                    if len(hyp[\"base_details\"]) == 0 or hyp[\"base_details\"][0] == 0:\n",
    "                        hyp_utility_f1.append(0)\n",
    "                        hyp_utility_f3.append(0)\n",
    "                        continue\n",
    "                    ### end of filtering baseline\n",
    "                elif dataset == \"humaneval\":\n",
    "                    ### add the filtering baseline\n",
    "                    if errors[task_id][hyp_id][\"base\"][\"status\"] != \"pass\":\n",
    "                        hyp_utility_f1.append(0)\n",
    "                        hyp_utility_f3.append(0)\n",
    "                        continue\n",
    "                    ### end of filtering baseline\n",
    "            \n",
    "            util_score_f1 = utility_f1[task_id][hyp_id][ref_id]\n",
    "            util_score_f3 = utility_f3[task_id][hyp_id][ref_id]\n",
    "            hyp_utility_f1.append(util_score_f1)\n",
    "            hyp_utility_f3.append(util_score_f3)\n",
    "        task_utility_f1.append(np.mean(hyp_utility_f1))\n",
    "        task_utility_f3.append(np.mean(hyp_utility_f3))\n",
    "        \n",
    "    \n",
    "    # get argmax\n",
    "    argmax_f1 = np.argmax(task_utility_f1) + start_id\n",
    "    argmax_f3 = np.argmax(task_utility_f3) + start_id\n",
    "    assert argmax_f1 == int(eval_results_dict[\"eval\"][task_id][argmax_f1][\"solution_id\"])\n",
    "    assert argmax_f3 == int(eval_results_dict[\"eval\"][task_id][argmax_f3][\"solution_id\"])\n",
    "    f1_base_status = eval_results_dict[\"eval\"][task_id][argmax_f1][\"base_status\"]\n",
    "    f3_base_status = eval_results_dict[\"eval\"][task_id][argmax_f3][\"base_status\"]\n",
    "    f1_plus_status = eval_results_dict[\"eval\"][task_id][argmax_f1][\"plus_status\"]\n",
    "    f3_plus_status = eval_results_dict[\"eval\"][task_id][argmax_f3][\"plus_status\"]\n",
    "\n",
    "    return (int(f1_base_status == \"pass\"), \n",
    "            int(f1_base_status == f1_plus_status == \"pass\"), \n",
    "            int(f3_base_status == \"pass\"), \n",
    "            int(f3_base_status == f3_plus_status == \"pass\"), \n",
    "            argmax_f1, argmax_f3)\n",
    "\n",
    "def get_mbr_results(eval_results, utility_f1, utility_f3, max_hyps=200, start_id=0, filter=True, workers=20):\n",
    "    for task_id in eval_results[\"eval\"]:\n",
    "        eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "        \n",
    "    f1_base_results = {}\n",
    "    f1_plus_results = {}\n",
    "    f3_base_results = {}\n",
    "    f3_plus_results = {}\n",
    "    argmax_f1_results = {}\n",
    "    argmax_f3_results = {}\n",
    "    \n",
    "    def process_single_task(task_id):\n",
    "        f1_base_pass, f1_plus_pass, f3_base_pass, f3_plus_pass, argmax_f1, argmax_f3 = process_task_mbr(task_id, \n",
    "                                                                                                eval_results, \n",
    "                                                                                                utility_f1, \n",
    "                                                                                                utility_f3, \n",
    "                                                                                                max_hyps=max_hyps, \n",
    "                                                                                                start_id=start_id, \n",
    "                                                                                                filter=filter)\n",
    "        f1_base_results[task_id] = f1_base_pass\n",
    "        f1_plus_results[task_id] = f1_plus_pass\n",
    "        f3_base_results[task_id] = f3_base_pass\n",
    "        f3_plus_results[task_id] = f3_plus_pass\n",
    "        argmax_f1_results[task_id] = argmax_f1\n",
    "        argmax_f3_results[task_id] = argmax_f3\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        list(tqdm(executor.map(process_single_task, eval_results[\"eval\"]), total=len(eval_results[\"eval\"])))\n",
    "        \n",
    "    for task_id in ape_eval_results[\"eval\"]:\n",
    "        ape_eval_results[\"eval\"][task_id] = sorted(ape_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "\n",
    "    f1_ape_base_results = {}\n",
    "    f1_ape_plus_results = {}\n",
    "    f3_ape_base_results = {}\n",
    "    f3_ape_plus_results = {}\n",
    "    for task_id in ape_eval_results[\"eval\"]:\n",
    "        argmax_f1_solution = ape_eval_results[\"eval\"][task_id][argmax_f1_results[task_id]]\n",
    "        argmax_f3_solution = ape_eval_results[\"eval\"][task_id][argmax_f3_results[task_id]]\n",
    "        argmax_f1_base_status = argmax_f1_solution[\"base_status\"]\n",
    "        argmax_f1_plus_status = argmax_f1_solution[\"plus_status\"]\n",
    "        argmax_f3_base_status = argmax_f3_solution[\"base_status\"]\n",
    "        argmax_f3_plus_status = argmax_f3_solution[\"plus_status\"]\n",
    "        assert int(argmax_f1_solution[\"solution_id\"]) == argmax_f1_results[task_id]\n",
    "        assert int(argmax_f3_solution[\"solution_id\"]) == argmax_f3_results[task_id]\n",
    "        f1_ape_base_results[task_id] = int(argmax_f1_base_status == \"pass\")\n",
    "        f1_ape_plus_results[task_id] = int(argmax_f1_base_status == argmax_f1_plus_status == \"pass\")\n",
    "        f3_ape_base_results[task_id] = int(argmax_f3_base_status == \"pass\")\n",
    "        f3_ape_plus_results[task_id] = int(argmax_f3_base_status == argmax_f3_plus_status == \"pass\")\n",
    "    \n",
    "    return (f1_base_results, \n",
    "            f1_plus_results, \n",
    "            f3_base_results, \n",
    "            f3_plus_results, \n",
    "            f1_ape_base_results, \n",
    "            f1_ape_plus_results, \n",
    "            f3_ape_base_results, \n",
    "            f3_ape_plus_results,\n",
    "            argmax_f1_results,\n",
    "            argmax_f3_results)\n",
    "\n",
    "def get_debugged_mbr_results(ape_eval_results, utility_f1, utility_f3, max_hyps=200, start_id=0, filter=True, workers=20):\n",
    "\n",
    "    f1_debugged_base_results = {}\n",
    "    f1_debugged_plus_results = {}\n",
    "    f3_debugged_base_results = {}\n",
    "    f3_debugged_plus_results = {}\n",
    "    argmax_f1_debugged_results = {}\n",
    "    argmax_f3_debugged_results = {}\n",
    "\n",
    "    def process_single_task(task_id):\n",
    "        (f1_debugged_base_result, \n",
    "         f1_debugged_plus_result, \n",
    "         f3_debugged_base_result, \n",
    "         f3_debugged_plus_result, \n",
    "         argmax_f1_debugged_result, \n",
    "         argmax_f3_debugged_result) = process_task_mbr(task_id, \n",
    "                                                    ape_eval_results, \n",
    "                                                    utility_f1,\n",
    "                                                    utility_f3,\n",
    "                                                    max_hyps=max_hyps, \n",
    "                                                    start_id=start_id, \n",
    "                                                    filter=filter)\n",
    "        f1_debugged_base_results[task_id] = f1_debugged_base_result\n",
    "        f1_debugged_plus_results[task_id] = f1_debugged_plus_result\n",
    "        f3_debugged_base_results[task_id] = f3_debugged_base_result\n",
    "        f3_debugged_plus_results[task_id] = f3_debugged_plus_result\n",
    "        argmax_f1_debugged_results[task_id] = argmax_f1_debugged_result\n",
    "        argmax_f3_debugged_results[task_id] = argmax_f3_debugged_result\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        list(tqdm(executor.map(process_single_task, ape_eval_results[\"eval\"]), total=len(ape_eval_results[\"eval\"])))\n",
    "\n",
    "    for task_id in ape_eval_results[\"eval\"]:\n",
    "        ape_eval_results[\"eval\"][task_id] = sorted(ape_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "\n",
    "    return (f1_debugged_base_results, \n",
    "            f1_debugged_plus_results, \n",
    "            f3_debugged_base_results, \n",
    "            f3_debugged_plus_results, \n",
    "            argmax_f1_debugged_results, \n",
    "            argmax_f3_debugged_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:00<00:00, 153844.38it/s]\n",
      "100%|██████████| 395/395 [00:00<00:00, 135743.55it/s]\n",
      "100%|██████████| 395/395 [00:00<00:00, 155257.25it/s]\n",
      "100%|██████████| 395/395 [00:00<00:00, 354992.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "num_plus_test_cases = 300\n",
    "max_hyps = 50\n",
    "filter = False\n",
    "workers = 40\n",
    "f1_base_score = []\n",
    "f1_plus_score = []\n",
    "f3_base_score = []\n",
    "f3_plus_score = []\n",
    "f1_ape_base_score = []\n",
    "f1_ape_plus_score = []\n",
    "f3_ape_base_score = []\n",
    "f3_ape_plus_score = []\n",
    "f1_debugged_base_score = []\n",
    "f1_debugged_plus_score = []\n",
    "f3_debugged_base_score = []\n",
    "f3_debugged_plus_score = []\n",
    "for start_id in range(0, 200, max_hyps):\n",
    "    # if modulo of start_id is 0, skip\n",
    "    if (start_id + 1/2 * max_hyps) % 50 == 0:\n",
    "        print(f\"skipping start_id: {start_id}\")\n",
    "        continue\n",
    "    (f1_base_results, \n",
    "     f1_plus_results, \n",
    "     f3_base_results, \n",
    "     f3_plus_results, \n",
    "     f1_ape_base_results, \n",
    "     f1_ape_plus_results, \n",
    "     f3_ape_base_results, \n",
    "     f3_ape_plus_results,\n",
    "     argmax_f1_results,\n",
    "     argmax_f3_results) = get_mbr_results(eval_results, \n",
    "                                        cbs_f1,\n",
    "                                        cbs_f3,\n",
    "                                        max_hyps=max_hyps, \n",
    "                                        start_id=start_id, \n",
    "                                        filter=filter,\n",
    "                                        workers=workers)\n",
    "    #(f1_debugged_base_results, \n",
    "    # f1_debugged_plus_results, \n",
    "    # f3_debugged_base_results, \n",
    "    # f3_debugged_plus_results, \n",
    "    # argmax_f1_debugged_results, \n",
    "    # argmax_f3_debugged_results) = get_debugged_mbr_results(ape_eval_results, \n",
    "    #                                                        ape_cbs_f1,\n",
    "    #                                                        ape_cbs_f3,\n",
    "    #                                                        max_hyps=max_hyps, \n",
    "    #                                                        start_id=start_id, \n",
    "    #                                                        filter=filter,\n",
    "    #                                                        workers=workers)\n",
    "    \n",
    "    f1_base_score.append(sum(f1_base_results.values())/len(f1_base_results))\n",
    "    f1_plus_score.append(sum(f1_plus_results.values())/len(f1_plus_results))\n",
    "    f3_base_score.append(sum(f3_base_results.values())/len(f3_base_results))\n",
    "    f3_plus_score.append(sum(f3_plus_results.values())/len(f3_plus_results))\n",
    "    f1_ape_base_score.append(sum(f1_ape_base_results.values())/len(f1_ape_base_results))\n",
    "    f1_ape_plus_score.append(sum(f1_ape_plus_results.values())/len(f1_ape_plus_results))\n",
    "    f3_ape_base_score.append(sum(f3_ape_base_results.values())/len(f3_ape_base_results))\n",
    "    f3_ape_plus_score.append(sum(f3_ape_plus_results.values())/len(f3_ape_plus_results))\n",
    "    #f1_debugged_base_score.append(sum(f1_debugged_base_results.values())/len(f1_debugged_base_results))\n",
    "    #f1_debugged_plus_score.append(sum(f1_debugged_plus_results.values())/len(f1_debugged_plus_results))\n",
    "    #f3_debugged_base_score.append(sum(f3_debugged_base_results.values())/len(f3_debugged_base_results))\n",
    "    #f3_debugged_plus_score.append(sum(f3_debugged_plus_results.values())/len(f3_debugged_plus_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: code-llama-13b-instruct_temp_1.6\n",
      "filter: False\n",
      "number of hypotheses: 50\n",
      "F1 MBR base      64.43038 %\n",
      "F1 MBR->APE base 68.79747 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "F1 MBR plus      53.5443 %\n",
      "F1 MBR->APE plus 56.89873 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "F3 MBR base      60.25316 %\n",
      "F3 MBR->APE base 67.21519 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "F3 MBR plus      50.12658 %\n",
      "F3 MBR->APE plus 56.01266 %\n"
     ]
    }
   ],
   "source": [
    "round_digits = 5\n",
    "print(\"model: {}\".format(gen_dir))\n",
    "print(\"filter: {}\".format(filter))\n",
    "print(\"number of hypotheses: {}\".format(max_hyps))\n",
    "print(\"F1 MBR base     \", np.round(sum(f1_base_score)/len(f1_base_score) * 100, round_digits), \"%\")\n",
    "print(\"F1 MBR->APE base\", np.round(sum(f1_ape_base_score)/len(f1_ape_base_score) * 100, round_digits), \"%\")\n",
    "#print(\"F1 APE->MBR base\", np.round(sum(f1_debugged_base_score)/len(f1_debugged_base_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"F1 MBR plus     \", np.round(sum(f1_plus_score)/len(f1_plus_score) * 100, round_digits), \"%\")\n",
    "print(\"F1 MBR->APE plus\", np.round(sum(f1_ape_plus_score)/len(f1_ape_plus_score) * 100, round_digits), \"%\")\n",
    "#print(\"F1 APE->MBR plus\", np.round(sum(f1_debugged_plus_score)/len(f1_debugged_plus_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"F3 MBR base     \", np.round(sum(f3_base_score)/len(f3_base_score) * 100, round_digits), \"%\")\n",
    "print(\"F3 MBR->APE base\", np.round(sum(f3_ape_base_score)/len(f3_ape_base_score) * 100, round_digits), \"%\")\n",
    "#print(\"F3 APE->MBR base\", np.round(sum(f3_debugged_base_score)/len(f3_debugged_base_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"F3 MBR plus     \", np.round(sum(f3_plus_score)/len(f3_plus_score) * 100, round_digits), \"%\")\n",
    "print(\"F3 MBR->APE plus\", np.round(sum(f3_ape_plus_score)/len(f3_ape_plus_score) * 100, round_digits), \"%\")\n",
    "#print(\"F3 APE->MBR plus\", np.round(sum(f3_debugged_plus_score)/len(f3_debugged_plus_score) * 100, round_digits), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/code_score.pkl\", \"rb\") as f:\n",
    "    code_score = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 129688.13it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 125706.48it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 282027.82it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 157082.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "num_plus_test_cases = 300\n",
    "max_hyps = 50\n",
    "filter = True\n",
    "workers = 40\n",
    "f1_base_score = []\n",
    "f1_plus_score = []\n",
    "f3_base_score = []\n",
    "f3_plus_score = []\n",
    "f1_ape_base_score = []\n",
    "f1_ape_plus_score = []\n",
    "f3_ape_base_score = []\n",
    "f3_ape_plus_score = []\n",
    "f1_debugged_base_score = []\n",
    "f1_debugged_plus_score = []\n",
    "f3_debugged_base_score = []\n",
    "f3_debugged_plus_score = []\n",
    "for start_id in range(0, 200, max_hyps):\n",
    "    # if modulo of start_id is 0, skip\n",
    "    if (start_id + 1/2 * max_hyps) % 50 == 0:\n",
    "        print(f\"skipping start_id: {start_id}\")\n",
    "        continue\n",
    "    (f1_base_results, \n",
    "     f1_plus_results, \n",
    "     f3_base_results, \n",
    "     f3_plus_results, \n",
    "     f1_ape_base_results, \n",
    "     f1_ape_plus_results, \n",
    "     f3_ape_base_results, \n",
    "     f3_ape_plus_results,\n",
    "     argmax_f1_results,\n",
    "     argmax_f3_results) = get_mbr_results(eval_results, \n",
    "                                        code_score,\n",
    "                                        code_score,\n",
    "                                        max_hyps=max_hyps, \n",
    "                                        start_id=start_id, \n",
    "                                        filter=filter,\n",
    "                                        workers=workers)\n",
    "    #(f1_debugged_base_results, \n",
    "    # f1_debugged_plus_results, \n",
    "    # f3_debugged_base_results, \n",
    "    # f3_debugged_plus_results, \n",
    "    # argmax_f1_debugged_results, \n",
    "    # argmax_f3_debugged_results) = get_debugged_mbr_results(ape_eval_results, \n",
    "    #                                                        ape_cbs_f1,\n",
    "    #                                                        ape_cbs_f3,\n",
    "    #                                                        max_hyps=max_hyps, \n",
    "    #                                                        start_id=start_id, \n",
    "    #                                                        filter=filter,\n",
    "    #                                                        workers=workers)\n",
    "    \n",
    "    f1_base_score.append(sum(f1_base_results.values())/len(f1_base_results))\n",
    "    f1_plus_score.append(sum(f1_plus_results.values())/len(f1_plus_results))\n",
    "    f3_base_score.append(sum(f3_base_results.values())/len(f3_base_results))\n",
    "    f3_plus_score.append(sum(f3_plus_results.values())/len(f3_plus_results))\n",
    "    f1_ape_base_score.append(sum(f1_ape_base_results.values())/len(f1_ape_base_results))\n",
    "    f1_ape_plus_score.append(sum(f1_ape_plus_results.values())/len(f1_ape_plus_results))\n",
    "    f3_ape_base_score.append(sum(f3_ape_base_results.values())/len(f3_ape_base_results))\n",
    "    f3_ape_plus_score.append(sum(f3_ape_plus_results.values())/len(f3_ape_plus_results))\n",
    "    #f1_debugged_base_score.append(sum(f1_debugged_base_results.values())/len(f1_debugged_base_results))\n",
    "    #f1_debugged_plus_score.append(sum(f1_debugged_plus_results.values())/len(f1_debugged_plus_results))\n",
    "    #f3_debugged_base_score.append(sum(f3_debugged_base_results.values())/len(f3_debugged_base_results))\n",
    "    #f3_debugged_plus_score.append(sum(f3_debugged_plus_results.values())/len(f3_debugged_plus_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: code-llama-13b-instruct_temp_1.6\n",
      "filter: True\n",
      "number of hypotheses: 50\n",
      "CS MBR base      76.98171 %\n",
      "CS MBR->APE base 77.13415 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CS MBR plus      65.70122 %\n",
      "CS MBR->APE plus 65.85366 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CS MBR base      76.98171 %\n",
      "CS MBR->APE base 77.13415 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CS MBR plus      65.70122 %\n",
      "CS MBR->APE plus 65.85366 %\n"
     ]
    }
   ],
   "source": [
    "round_digits = 5\n",
    "print(\"model: {}\".format(gen_dir))\n",
    "print(\"filter: {}\".format(filter))\n",
    "print(\"number of hypotheses: {}\".format(max_hyps))\n",
    "print(\"CS MBR base     \", np.round(sum(f1_base_score)/len(f1_base_score) * 100, round_digits), \"%\")\n",
    "print(\"CS MBR->APE base\", np.round(sum(f1_ape_base_score)/len(f1_ape_base_score) * 100, round_digits), \"%\")\n",
    "#print(\"F1 APE->MBR base\", np.round(sum(f1_debugged_base_score)/len(f1_debugged_base_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"CS MBR plus     \", np.round(sum(f1_plus_score)/len(f1_plus_score) * 100, round_digits), \"%\")\n",
    "print(\"CS MBR->APE plus\", np.round(sum(f1_ape_plus_score)/len(f1_ape_plus_score) * 100, round_digits), \"%\")\n",
    "#print(\"F1 APE->MBR plus\", np.round(sum(f1_debugged_plus_score)/len(f1_debugged_plus_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"CS MBR base     \", np.round(sum(f3_base_score)/len(f3_base_score) * 100, round_digits), \"%\")\n",
    "print(\"CS MBR->APE base\", np.round(sum(f3_ape_base_score)/len(f3_ape_base_score) * 100, round_digits), \"%\")\n",
    "#print(\"F3 APE->MBR base\", np.round(sum(f3_debugged_base_score)/len(f3_debugged_base_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"CS MBR plus     \", np.round(sum(f3_plus_score)/len(f3_plus_score) * 100, round_digits), \"%\")\n",
    "print(\"CS MBR->APE plus\", np.round(sum(f3_ape_plus_score)/len(f3_ape_plus_score) * 100, round_digits), \"%\")\n",
    "#print(\"F3 APE->MBR plus\", np.round(sum(f3_debugged_plus_score)/len(f3_debugged_plus_score) * 100, round_digits), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

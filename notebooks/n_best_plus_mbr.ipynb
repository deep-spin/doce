{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from warnings import warn\n",
    "\n",
    "from multiprocessing import Value\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from evalplus.data.utils import CACHE_DIR\n",
    "\n",
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "\n",
    "from evalplus.eval.utils import (\n",
    "    create_tempdir,\n",
    "    reliability_guard,\n",
    "    swallow_io,\n",
    "    time_limit,\n",
    ")\n",
    "\n",
    "from evalplus.eval._special_oracle import (\n",
    "    MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    MBPP_OUTPUT_SET_EQ_TASKS,\n",
    "    _poly,\n",
    ")\n",
    "\n",
    "from evalplus.gen.util import trusted_exec\n",
    "\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False\n",
    "\n",
    "import resource\n",
    "import signal\n",
    "\n",
    "def time_limit_exceeded(signum, frame):\n",
    "    raise TimeoutError(\"Time limit exceeded\")\n",
    "\n",
    "def set_time_limit(seconds):\n",
    "    signal.signal(signal.SIGALRM, time_limit_exceeded)\n",
    "    signal.alarm(seconds)\n",
    "\n",
    "def set_memory_limit(maximum_memory_bytes):\n",
    "    import resource\n",
    "\n",
    "    resource.setrlimit(\n",
    "        resource.RLIMIT_AS, (maximum_memory_bytes, maximum_memory_bytes)\n",
    "    )\n",
    "    resource.setrlimit(\n",
    "        resource.RLIMIT_DATA, (maximum_memory_bytes, maximum_memory_bytes)\n",
    "    )\n",
    "    if not platform.uname().system == \"Darwin\":\n",
    "        resource.setrlimit(\n",
    "            resource.RLIMIT_STACK, (maximum_memory_bytes, maximum_memory_bytes)\n",
    "        )\n",
    "\n",
    "def wrapped_ut_exact_match(\n",
    "    hyp_ut, \n",
    "    ref_ut, \n",
    "    entry_point, \n",
    "    dataset, \n",
    "    inp=None, \n",
    "    atol=0, \n",
    "    time_bound = False,\n",
    "    time_limit=1, \n",
    "    memory_bound = True,\n",
    "    memory_limit=40*1024*1024, \n",
    "    ):\n",
    "    final_result = Value('d', 0)\n",
    "    with create_tempdir():\n",
    "        # These system calls are needed when cleaning up tempdir.\n",
    "        import os\n",
    "        import shutil\n",
    "\n",
    "        rmtree = shutil.rmtree\n",
    "        rmdir = os.rmdir\n",
    "        chdir = os.chdir\n",
    "        if time_bound:\n",
    "            set_time_limit(time_limit)\n",
    "        try:\n",
    "            if memory_bound:\n",
    "                reliability_guard(maximum_memory_bytes=memory_limit)\n",
    "            if time_bound:\n",
    "                set_time_limit(time_limit)\n",
    "            final_result = ut_exact_match(hyp_ut, ref_ut, entry_point, dataset, inp, atol, time_limit, memory_limit, memory_bound)\n",
    "        except MemoryError:\n",
    "            final_result = 0.0\n",
    "        except TimeoutError:\n",
    "            final_result = 0.0\n",
    "    if time_bound:\n",
    "        signal.alarm(0)\n",
    "    shutil.rmtree = rmtree\n",
    "    os.rmdir = rmdir\n",
    "    os.chdir = chdir\n",
    "    return final_result.value\n",
    "\n",
    "def ut_exact_match(\n",
    "    hyp_ut, \n",
    "    ref_ut, \n",
    "    entry_point, \n",
    "    dataset, \n",
    "    inp=None, \n",
    "    atol=0, # need to change this later\n",
    "    time_limit=1, # seconds\n",
    "    memory_limit=4*1024*1024*1024, # 4GB\n",
    "    ):\n",
    "    \n",
    "    #try:\n",
    "        #set_time_limit(time_limit)\n",
    "        #set_memory_limit(memory_limit)\n",
    "\n",
    "    exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    # ================================================ #\n",
    "    # ============== special oracles ================= #\n",
    "    if dataset == \"mbpp\":\n",
    "        if \"are_equivalent\" == entry_point:  # Mbpp/164 special oracle\n",
    "            exact_match = exact_match or True\n",
    "        elif \"sum_div\" == entry_point:  # Mbpp/295 special oracle\n",
    "            exact_match = exact_match or hyp_ut == 0 or ref_ut == 0\n",
    "        elif entry_point in MBPP_OUTPUT_SET_EQ_TASKS:\n",
    "            exact_match = set(hyp_ut) == set(ref_ut)\n",
    "        elif entry_point in MBPP_OUTPUT_NOT_NONE_TASKS:\n",
    "            # exp is True  if not None\n",
    "            #        False if None\n",
    "            if isinstance(hyp_ut, bool):\n",
    "                hyp_ut = hyp_ut is not None\n",
    "            if isinstance(ref_ut, bool):\n",
    "                ref_ut = ref_ut is not None\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    if dataset == \"humaneval\":\n",
    "        if \"find_zero\" == entry_point:\n",
    "            hyp_ut = _poly(*inp, hyp_ut) <= atol\n",
    "            ref_ut = _poly(*inp, ref_ut) <= atol\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "    # ============== special oracles ================= #\n",
    "    # ================================================ #\n",
    "\n",
    "    if atol == 0 and (is_floats(ref_ut) or is_floats(hyp_ut)):\n",
    "        atol = 1e-6  # enforce atol for float comparison\n",
    "    if not exact_match and atol != 0:\n",
    "        # explicitly set rtol=1e-07\n",
    "        # to match `np.testing.assert_allclose`'s default values\n",
    "        exact_match =  np.allclose(hyp_ut, ref_ut, rtol=1e-07, atol=atol)\n",
    "    \n",
    "    return int(exact_match)\n",
    "\n",
    "def get_groundtruth(problems, hashcode, tasks_only_output_not_none):\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{hashcode}.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        #print(f\"Load from ground-truth from {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    #print(\"Computing expected output...\")\n",
    "    tbegin = time.time()\n",
    "    expected_output = {}\n",
    "    for task_id, problem in problems.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"base_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"plus_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "        expected_output[task_id] = oracle\n",
    "    #print(f\"Expected outputs computed in {time.time() - tbegin:.2f}s\")\n",
    "\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(expected_output, f)\n",
    "\n",
    "    return expected_output\n",
    "\n",
    "def mbr_exec(hyp_uts, ref_uts, entry_point, dataset, n_uts, inps=None, granular=False):\n",
    "    n_matches = 0\n",
    "    for i in range(n_uts):\n",
    "        # skip if either hyp_ut or ref_ut is not in the list\n",
    "        if i not in hyp_uts or i not in ref_uts:\n",
    "            continue\n",
    "        # if there's an error, we return 0\n",
    "        if type(hyp_uts[i]) == str and hyp_uts[i].startswith(\"failed:\"):\n",
    "            return 0 \n",
    "        if type(ref_uts[i]) == str and ref_uts[i].startswith(\"failed:\"):\n",
    "            return 0\n",
    "        # we start counting the number of matches\n",
    "        try:\n",
    "            n_matches += ut_exact_match(\n",
    "                hyp_uts[i], \n",
    "                ref_uts[i], \n",
    "                entry_point, \n",
    "                dataset, \n",
    "                inp=inps[i] if inps else None\n",
    "                )\n",
    "        except:\n",
    "            n_matches += 0\n",
    "        \n",
    "    if granular:\n",
    "        try:\n",
    "            return n_matches/ n_uts\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        return int(n_matches == n_uts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec_outputs loaded\n",
      "eval_results loaded\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs\"\n",
    "dataset = \"mbpp\"\n",
    "gen_dir = \"code-llama-13b-instruct_temp_1.6\"\n",
    "#gen_dir = \"deepseek-coder-6.7b-instruct_temp_1.2\"\n",
    "#debug_gen_dir = gen_dir + \"_debug1_not_change_positive\"\n",
    "#debug_gen_dir = gen_dir + \"_debug1_sd-ut\"\n",
    "#debug_3times_gen_dir = gen_dir + \"_debug1_sd-ut\"\n",
    "# load exec_outputs\n",
    "\n",
    "# load problems\n",
    "if dataset == \"mbpp\":\n",
    "    problems = get_mbpp_plus()\n",
    "    dataset_hash = get_mbpp_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    )\n",
    "elif dataset == \"humaneval\":\n",
    "    problems = get_human_eval_plus()\n",
    "    dataset_hash = get_human_eval_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        []\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n",
    "\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/exec_outputs_v2.pkl\", \"rb\") as f:\n",
    "    exec_outputs = pickle.load(f)\n",
    "print(\"exec_outputs loaded\")\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "    \n",
    "#pop out [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]\n",
    "if dataset == \"mbpp\":\n",
    "    for task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        eval_results[\"eval\"].pop(task_id)\n",
    "print(\"eval_results loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs loaded\n",
      "reviewer_logprobs loaded\n"
     ]
    }
   ],
   "source": [
    "if dataset == \"humaneval\":\n",
    "    with open(f\"{work_dir}/{dataset}/{gen_dir}/errors.pkl\", \"rb\") as f:\n",
    "        errors = pickle.load(f)\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/logprobs.pkl\", \"rb\") as f:\n",
    "    logprobs = pickle.load(f)\n",
    "print(\"logprobs loaded\")\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/reviewer_logprobs.pkl\", \"rb\") as f:\n",
    "    reviewer_logprobs = pickle.load(f)\n",
    "print(\"reviewer_logprobs loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_task(task_id, eval_results_dict, exec_outputs_dict, max_hyps = 200, start_id = 0, num_plus_test_cases=3, granular=False, filter = True):\n",
    "    p_name = task_id.replace(\"/\", \"_\")\n",
    "    test_all_plus_cases = True\n",
    "    assert start_id >= 0 and start_id + max_hyps <= 200\n",
    "    n_expected_outputs_base = len(expected_output[task_id][\"base\"])\n",
    "    n_expected_outputs_plus = len(expected_output[task_id][\"plus\"])\n",
    "    if num_plus_test_cases < n_expected_outputs_plus:\n",
    "        test_all_plus_cases = False\n",
    "    task_utility_base = []\n",
    "    task_utility_plus = []\n",
    "    \n",
    "    for hyp_id, hyp in enumerate(eval_results_dict[\"eval\"][task_id]):\n",
    "        if hyp_id >= max_hyps + start_id or hyp_id < start_id:\n",
    "            continue\n",
    "        hyp_base_outputs = exec_outputs_dict[task_id][hyp_id][\"base\"]\n",
    "        hyp_plus_outputs = exec_outputs_dict[task_id][hyp_id][\"plus\"]\n",
    "        if not test_all_plus_cases:\n",
    "            hyp_plus_outputs = {i: hyp_plus_outputs[i] for i in range(num_plus_test_cases) if i < min(num_plus_test_cases, len(hyp_plus_outputs))}\n",
    "        hyp_utility_base = []\n",
    "        hyp_utility_plus = []\n",
    "        \n",
    "        for ref_id, ref in enumerate(eval_results_dict[\"eval\"][task_id]):\n",
    "            if ref_id >= max_hyps + start_id or ref_id < start_id:\n",
    "                continue\n",
    "            if ref[\"base_status\"] == ref[\"plus_status\"] == hyp[\"base_status\"] == hyp[\"plus_status\"] == \"pass\":\n",
    "                hyp_utility_base.append(1)\n",
    "                hyp_utility_plus.append(1)\n",
    "                continue\n",
    "            if filter:\n",
    "                ### add the filtering baseline\n",
    "                if dataset in \"humaneval\":\n",
    "                    if errors[task_id][hyp_id][\"base\"][\"status\"] != \"pass\":\n",
    "                        hyp_utility_base.append(0)\n",
    "                        hyp_utility_plus.append(0)\n",
    "                        continue\n",
    "                elif dataset == \"mbpp\":\n",
    "                    if len(hyp[\"base_details\"]) == 0 or hyp[\"base_details\"][0] == 0:\n",
    "                        hyp_utility_base.append(0)\n",
    "                        hyp_utility_plus.append(0)\n",
    "                        continue\n",
    "                ### end of filtering baseline\n",
    "            ref_base_outputs = exec_outputs_dict[task_id][ref_id][\"base\"]\n",
    "            ref_plus_outputs = exec_outputs_dict[task_id][ref_id][\"plus\"]\n",
    "            if not test_all_plus_cases:\n",
    "                ref_plus_outputs = {i: ref_plus_outputs[i] for i in range(num_plus_test_cases) if i < min(num_plus_test_cases, len(ref_plus_outputs))}\n",
    "            \n",
    "            util_score_base = mbr_exec(hyp_base_outputs, ref_base_outputs, problems[task_id][\"entry_point\"], \"mbpp\", n_expected_outputs_base, granular=granular)\n",
    "            util_score_plus = mbr_exec(hyp_plus_outputs, ref_plus_outputs, problems[task_id][\"entry_point\"], \"mbpp\", n_expected_outputs_plus, granular=granular)\n",
    "            hyp_utility_base.append(util_score_base)\n",
    "            if granular:\n",
    "                hyp_utility_plus.append((util_score_plus*n_expected_outputs_plus+util_score_base*n_expected_outputs_base)/(n_expected_outputs_plus+n_expected_outputs_base))\n",
    "            else:\n",
    "                hyp_utility_plus.append(int(util_score_plus==util_score_base==1))\n",
    "            #hyp_utility_plus.append(mbr_exec(hyp_plus_outputs, ref_plus_outputs, problems[task_id][\"entry_point\"], \"mbpp\", n_expected_outputs_plus))\n",
    "            #hyp_utility_plus.extend(hyp_utility_base)\n",
    "        task_utility_base.append(np.mean(hyp_utility_base) \n",
    "                                 * np.exp(\n",
    "                                     np.mean(reviewer_logprobs[p_name][hyp_id]) +\n",
    "                                     np.mean(logprobs[p_name][hyp_id])\n",
    "                                     )\n",
    "                                 )\n",
    "        task_utility_plus.append(np.mean(hyp_utility_plus) \n",
    "                                 * np.exp(\n",
    "                                     np.mean(reviewer_logprobs[p_name][hyp_id]) +\n",
    "                                     np.mean(logprobs[p_name][hyp_id])\n",
    "                                     )\n",
    "                                 )\n",
    "    \n",
    "    # get argmax\n",
    "    argmax_base = np.argmax(task_utility_base) + start_id\n",
    "    argmax_plus = np.argmax(task_utility_plus) + start_id\n",
    "    assert argmax_base == int(eval_results_dict[\"eval\"][task_id][argmax_base][\"solution_id\"])\n",
    "    assert argmax_plus == int(eval_results_dict[\"eval\"][task_id][argmax_plus][\"solution_id\"])\n",
    "    base_status = eval_results_dict[\"eval\"][task_id][argmax_base][\"base_status\"]\n",
    "    plus_status = eval_results_dict[\"eval\"][task_id][argmax_plus][\"plus_status\"]\n",
    "    if not test_all_plus_cases:\n",
    "        argmax_plus_solution = eval_results_dict[\"eval\"][task_id][argmax_plus]\n",
    "        if len(argmax_plus_solution[\"plus_details\"]) < min(num_plus_test_cases, len(expected_output[task_id][\"plus\"])):\n",
    "            plus_status = \"fail\"\n",
    "        else:\n",
    "            if all(argmax_plus_solution[\"plus_details\"][:num_plus_test_cases]):\n",
    "                plus_status = \"pass\"\n",
    "            else:\n",
    "                plus_status = \"fail\"\n",
    "    return (int(base_status == \"pass\"), int(base_status == plus_status == \"pass\"), argmax_base, argmax_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(eval_results, exec_outputs, max_hyps=200, start_id=0, num_plus_test_cases=300, granular=False, filter=True, workers=20):\n",
    "    for task_id in eval_results[\"eval\"]:\n",
    "        eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "        \n",
    "    base_results = {}\n",
    "    plus_results = {}\n",
    "    argmax_bases = {}\n",
    "    argmax_pluss = {}\n",
    "    \n",
    "    def process_single_task(task_id):\n",
    "        base_result, plus_result, argmax_base, argmax_plus = process_task(task_id, \n",
    "                                                                          eval_results, \n",
    "                                                                          exec_outputs, \n",
    "                                                                          max_hyps=max_hyps, \n",
    "                                                                          start_id=start_id, \n",
    "                                                                          num_plus_test_cases=num_plus_test_cases, \n",
    "                                                                          granular=granular, \n",
    "                                                                          filter=filter)\n",
    "        base_results[task_id] = base_result\n",
    "        plus_results[task_id] = plus_result\n",
    "        argmax_bases[task_id] = argmax_base\n",
    "        argmax_pluss[task_id] = argmax_plus\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        list(tqdm(executor.map(process_single_task, eval_results[\"eval\"]), total=len(eval_results[\"eval\"])))\n",
    "    \n",
    "    return base_results, plus_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/395 [00:00<00:03, 98.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 69.29it/s] \n",
      "100%|██████████| 395/395 [00:05<00:00, 71.02it/s] \n",
      "100%|██████████| 395/395 [00:05<00:00, 74.34it/s]\n",
      "100%|██████████| 395/395 [00:05<00:00, 67.74it/s] \n"
     ]
    }
   ],
   "source": [
    "num_plus_test_cases = 300\n",
    "max_hyps = 50\n",
    "granular = False\n",
    "filter = True\n",
    "workers = 1\n",
    "base_score = []\n",
    "plus_score = []\n",
    "for start_id in range(0, 200, max_hyps):\n",
    "    #the thing that dont pass is between 120 and 125s\n",
    "    #if start_id == 120:\n",
    "    #    continue\n",
    "    base_results, plus_results = get_results(eval_results, \n",
    "                                             exec_outputs, \n",
    "                                             max_hyps=max_hyps, \n",
    "                                             start_id=start_id, \n",
    "                                             num_plus_test_cases=num_plus_test_cases,\n",
    "                                             granular=granular,\n",
    "                                             filter=filter, \n",
    "                                             workers=workers)\n",
    "\n",
    "    base_score.append(sum(base_results.values())/len(base_results))\n",
    "    plus_score.append(sum(plus_results.values())/len(plus_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter: True\n",
      "task: mbpp\n",
      "model: code-llama-13b-instruct_temp_1.6\n",
      "number of hypotheses: 50\n",
      "MBR base      84.1772151899 %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MBR plus      74.8101265823 %\n"
     ]
    }
   ],
   "source": [
    "round_digits = 10\n",
    "print(\"filter: {}\".format(filter))\n",
    "print(\"task: {}\".format(dataset))\n",
    "print(\"model: {}\".format(gen_dir))\n",
    "print(\"number of hypotheses: {}\".format(max_hyps))\n",
    "print(\"MBR base     \", np.round(sum(base_score)/len(base_score) * 100, round_digits), \"%\")\n",
    "print(\"-\"*100)\n",
    "print(\"MBR plus     \", np.round(sum(plus_score)/len(plus_score) * 100, round_digits), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

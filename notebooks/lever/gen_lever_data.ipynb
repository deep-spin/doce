{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/haausing/miniconda3/envs/evalplus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"evalplus/mbppplus\")\n",
    "problems = get_mbpp_plus()\n",
    "for elem in dataset[\"test\"]:\n",
    "    task_id = \"Mbpp/{}\".format(elem[\"task_id\"])\n",
    "    problems[task_id][\"text\"] = elem[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file\n",
    "import json\n",
    "import pandas as pd\n",
    "def load_json(dir_path: str):\n",
    "    with open(dir_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "eval_results = load_json(\"../../evalplus_outputs/mbpp/deepseek-coder-7b-instruct-v1.5_temp_0.8/eval_results.json\")\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "log_probs = pd.read_pickle(\"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/mbpp/deepseek-coder-7b-instruct-v1.5_temp_0.8/logprobs.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['code', 'exec_match', 'linear_score', 'exec_result', 'gen_prob', 'norm_gen_prob']\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "for elem in dataset[\"test\"]:\n",
    "    task_id = \"Mbpp/{}\".format(elem[\"task_id\"])\n",
    "    base_input_counts = max([len(e[\"base_details\"]) for e in eval_results[\"eval\"][task_id]])\n",
    "    base_input_counts = max([1, base_input_counts])\n",
    "    plus_input_counts = max([len(e[\"plus_details\"]) for e in eval_results[\"eval\"][task_id]])\n",
    "    plus_input_counts = max([1, plus_input_counts])\n",
    "    metadata = {\"text\": problems[task_id][\"text\"]}\n",
    "    gold_program = {\n",
    "        \"code\": problems[task_id][\"canonical_solution\"], \n",
    "        \"exec_match\": 1, \n",
    "        \"linear_score\": 1.0, \n",
    "        \"exec_result\": [], \n",
    "        \"gen_prob\": 0, \n",
    "        \"norm_gen_prob\": 0}\n",
    "    generated_programs = [\n",
    "        {\n",
    "            \"code\": e[\"solution\"],\n",
    "            #\"exec_match\": (e[\"base_status\"] == e[\"plus_status\"] == \"pass\"),\n",
    "            \"exec_match\": (e[\"base_status\"] == \"pass\"),\n",
    "            #\"linear_score\": (sum(e[\"base_details\"]) + sum(e[\"plus_details\"])) / (base_input_counts + plus_input_counts),\n",
    "            \"linear_score\": sum(e[\"base_details\"]) / base_input_counts,\n",
    "            \"exec_result\": [],\n",
    "            \"gen_prob\": sum(log_probs[task_id.replace(\"/\",\"_\")][int(e[\"solution_id\"])]),\n",
    "            \"norm_gen_prob\": sum(log_probs[task_id.replace(\"/\",\"_\")][int(e[\"solution_id\"])])/len(log_probs[task_id.replace(\"/\",\"_\")][int(e[\"solution_id\"])]),\n",
    "        } for e in eval_results[\"eval\"][task_id]]\n",
    "    if int(elem[\"task_id\"]) > 600: # this is for the training partition\n",
    "        train_data.append({\"metadata\": metadata,\n",
    "                           \"gold_program\": gold_program,\n",
    "                           \"generated_programs\": generated_programs,})\n",
    "    elif int(elem[\"task_id\"]) > 510 and int(elem[\"task_id\"]) <=600: # this is for the development partition\n",
    "        dev_data.append({\"metadata\": metadata,\n",
    "                           \"gold_program\": gold_program,\n",
    "                           \"generated_programs\": generated_programs,})\n",
    "    elif int(elem[\"task_id\"]) > 10 and int(elem[\"task_id\"]) <= 510: # this is for the test partition\n",
    "        test_data.append({\"metadata\": metadata,\n",
    "                           \"gold_program\": gold_program,\n",
    "                           \"generated_programs\": generated_programs,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of dictionaries to jsonl file\n",
    "prefix = \"/mnt/scratch/haausing/code_reranking/code/lever/data/mbpp/\"\n",
    "import json\n",
    "with open(prefix + \"deepseek-coder-7b-instruct-v1.5_temp_0.8_base_train.jsonl\", \"w\") as f:\n",
    "    for elem in train_data:\n",
    "        f.write(json.dumps(elem) + \"\\n\")\n",
    "with open(prefix + \"deepseek-coder-7b-instruct-v1.5_temp_0.8_base_dev.jsonl\", \"w\") as f:\n",
    "    for elem in dev_data:\n",
    "        f.write(json.dumps(elem) + \"\\n\")\n",
    "with open(prefix + \"deepseek-coder-7b-instruct-v1.5_temp_0.8_base_test.jsonl\", \"w\") as f:\n",
    "    for elem in test_data:\n",
    "        f.write(json.dumps(elem) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "str(type(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

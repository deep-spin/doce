{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sample index of \"{\"\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "def get_info_and_exec(sample: str) -> dict:\n",
    "    try:\n",
    "        info = sample[:sample.index(\"{\")].strip()\n",
    "        task_id, solution_id, base_plus = info.split(\" \")\n",
    "    except:\n",
    "        task_id, solution_id, base_plus = sample.strip().split(\" \")\n",
    "        sample = \"{}\\n\"\n",
    "    try:\n",
    "        execution = eval(sample[sample.index(\"{\"):].strip())\n",
    "    except:\n",
    "        try:\n",
    "            execution = sample[sample.index(\"{\"):].strip().replace(\"-inf\", \"\\'minus_ifty\\'\").replace(\"inf\", \"\\'ifty\\'\")\n",
    "            execution = eval(execution)\n",
    "            for e in execution:\n",
    "                if execution[e] == \"minus_ifty\":\n",
    "                    execution[e] = float(\"-inf\")\n",
    "                elif execution[e] == \"ifty\":\n",
    "                    execution[e] = float(\"inf\")\n",
    "            if task_id in [\"758\", \"237\"] and solution_id in [\"55\", \"136\", \"149\"]:\n",
    "                print(sample)\n",
    "        except:\n",
    "            print(sample)\n",
    "            assert task_id in [\"758\", \"237\"]\n",
    "            assert solution_id in [\"55\", \"136\", \"149\"]\n",
    "            if task_id == \"758\":\n",
    "                if solution_id == 136:\n",
    "                    execution = {0: defaultdict(int, {(1, 3): 2, (5, 7): 2, (13, 15, 17): 1, (9, 11): 1}), \n",
    "                                 1: defaultdict(int, {('green', 'orange'): 2, ('black',): 1, ('white',): 1}), \n",
    "                                 2: defaultdict(int, {(10, 20, 30, 40): 1, (60, 70, 50, 50): 1, (90, 100, 200): 1}), \n",
    "                                 3: defaultdict(int, {('john',): 1})}\n",
    "                elif solution_id == 149:\n",
    "                    execution = {0: Counter({(1, 3): 2, (5, 7): 2, (13, 15, 17): 1, (9, 11): 1}), \n",
    "                                 1: Counter({('green', 'orange'): 2, ('black',): 1, ('white',): 1}), \n",
    "                                 2: Counter({(10, 20, 30, 40): 1, (50, 50, 60, 70): 1, (90, 100, 200): 1}), \n",
    "                                 3: Counter({('john',): 1})}\n",
    "            elif task_id == \"237\":\n",
    "                execution = {0: Counter({(3, 1): 1, (1, 3): 1, (2, 5): 1, (5, 2): 1, (6, 3): 1}), \n",
    "                             1: Counter({(4, 2): 1, (2, 4): 1, (3, 6): 1, (6, 3): 1, (7, 4): 1}), \n",
    "                             2: Counter({(13, 2): 1, (11, 23): 1, (12, 25): 1, (25, 12): 1, (16, 23): 1})}\n",
    "    return {\"task_id\": task_id, \"solution_id\": int(solution_id), \"base_plus\": base_plus, \"execution\": execution}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_outputs_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/mbpp/deepseek-coder-7b-instruct-v1.5_temp_0.8/exec_outputs_base_only.txt\"\n",
    "#read lines of exec_outputs_dir\n",
    "with open(exec_outputs_dir, \"r\") as f:\n",
    "    exec_outputs = [e for e in f.readlines() if e != \"\\n\"]\n",
    "exec_outputs = \"\".join(exec_outputs).split(\"Mbpp/\")[1:]\n",
    "exec_outputs[-1] = exec_outputs[-1].split(\"\\n\")[0]\n",
    "#exec_outputs = [e for e in exec_outputs if \"Mbpp\" in e]\n",
    "# count the number of \"Mbpp\" in exec_outputs\n",
    "#from tqdm import tqdm\n",
    "#for i,line in tqdm(enumerate(exec_outputs)):\n",
    "#    if line.count(\"Mbpp\") > 1:\n",
    "#        execs = [\"Mbpp\"+e for e in line.split(\"Mbpp\") if e != '']\n",
    "#        exec_outputs[i] = execs[0]\n",
    "#        exec_outputs.extend(execs[1:])\n",
    "#exec_outputs = [get_info_and_exec(e) for e in exec_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758 136 base {0: defaultdict(<class 'int'>, {(1, 3): 2, (5, 7): 2, (13, 15, 17): 1, (9, 11): 1}), 1: defaultdict(<class 'int'>, {('green', 'orange'): 2, ('black',): 1, ('white',): 1}), 2: defaultdict(<class 'int'>, {(10, 20, 30, 40): 1, (60, 70, 50, 50): 1, (90, 100, 200): 1}), 3: defaultdict(<class 'int'>, {('john',): 1})}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_exec_outputs = [get_info_and_exec(e) for e in exec_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by task_id, solution_id, base_plus\n",
    "new_exec_outputs = sorted(new_exec_outputs, key = lambda x: (int(x[\"task_id\"]), x[\"solution_id\"], x[\"base_plus\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec_outputs loaded\n",
      "eval_results loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs\"\n",
    "dataset = \"humaneval\"\n",
    "#gen_dir = \"code-llama-7b-instruct_temp_1.6\"\n",
    "gen_dir = \"deepseek-coder-7b-instruct-v1.5_temp_0.8\"\n",
    "\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/exec_outputs_v2.pkl\", \"rb\") as f:\n",
    "    exec_outputs = pickle.load(f)\n",
    "print(\"exec_outputs loaded\")\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "print(\"eval_results loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file of log probs\n",
    "#with open(f\"{work_dir}/{dataset}/{gen_dir}/logprobs_full.pkl\", \"rb\") as f:\n",
    "#    log_probs = pickle.load(f)\n",
    "#print(\"logprobs loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "\n",
    "from evalplus.eval._special_oracle import (\n",
    "    MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    MBPP_OUTPUT_SET_EQ_TASKS,\n",
    "    _poly,\n",
    ")\n",
    "\n",
    "from evalplus.data.utils import CACHE_DIR\n",
    "\n",
    "from evalplus.gen.util import trusted_exec\n",
    "\n",
    "def get_groundtruth(problems, hashcode, tasks_only_output_not_none):\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{hashcode}.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        #print(f\"Load from ground-truth from {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    #print(\"Computing expected output...\")\n",
    "    tbegin = time.time()\n",
    "    expected_output = {}\n",
    "    for task_id, problem in problems.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"base_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"plus_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "        expected_output[task_id] = oracle\n",
    "    #print(f\"Expected outputs computed in {time.time() - tbegin:.2f}s\")\n",
    "\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(expected_output, f)\n",
    "\n",
    "    return expected_output\n",
    "\n",
    "# load problems\n",
    "if dataset == \"mbpp\":\n",
    "    problems = get_mbpp_plus()\n",
    "    dataset_hash = get_mbpp_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    )\n",
    "elif dataset == \"humaneval\":\n",
    "    problems = get_human_eval_plus()\n",
    "    dataset_hash = get_human_eval_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        []\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_idx, sample_idx, result_unit_test_1, result_unit_test_2,  ..., result_unit_test_X\n",
    "final_outputs = []\n",
    "for task_id in exec_outputs:\n",
    "    #if task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        #continue\n",
    "    for solution_id in range(200):\n",
    "        try:\n",
    "            assert solution_id == int(eval_results[\"eval\"][task_id][solution_id][\"solution_id\"])\n",
    "            p_name = task_id.replace(\"/\", \"_\")\n",
    "            final_outputs.append(\n",
    "                {\n",
    "                    \"src_idx\": int(task_id.split(\"/\")[-1]),\n",
    "                    \"sample_idx\": solution_id,\n",
    "                    #\"log_prob\": sum(log_probs[p_name][solution_id]),\n",
    "                    #\"norm_log_prob\": sum(log_probs[p_name][solution_id])/len(log_probs[p_name][solution_id]),\n",
    "                    \"result_unit_test_1\": exec_outputs[task_id][solution_id][\"base\"][0] \n",
    "                        if 0 in exec_outputs[task_id][solution_id][\"base\"] else \"failed: no output\",\n",
    "                    \"result_unit_test_2\": exec_outputs[task_id][solution_id][\"base\"][1] \n",
    "                        if 1 in exec_outputs[task_id][solution_id][\"base\"] else \"failed: no output\",\n",
    "                    \"result_unit_test_3\": exec_outputs[task_id][solution_id][\"base\"][2] \n",
    "                        if 2 in exec_outputs[task_id][solution_id][\"base\"] else \"failed: no output\",\n",
    "                    \"ground_truth_test_1\": expected_output[task_id][\"base\"][0]\n",
    "                        if len(expected_output[task_id][\"base\"])>0 else \"failed: no output\",\n",
    "                    \"ground_truth_test_2\": expected_output[task_id][\"base\"][1]\n",
    "                        if len(expected_output[task_id][\"base\"])>1 else \"failed: no output\",\n",
    "                    \"ground_truth_test_3\": expected_output[task_id][\"base\"][2]\n",
    "                        if len(expected_output[task_id][\"base\"])>2 else \"failed: no output\",\n",
    "                    #\"ground_truth_test_1\": eval_results[\"eval\"][task_id][solution_id][\"base_details\"][0]==1 \n",
    "                    #    if len(eval_results[\"eval\"][task_id][solution_id][\"base_details\"])>0 else False,\n",
    "                    #\"ground_truth_test_2\": eval_results[\"eval\"][task_id][solution_id][\"base_details\"][1]==1 \n",
    "                    #    if len(eval_results[\"eval\"][task_id][solution_id][\"base_details\"])>1 else False,\n",
    "                    #\"ground_truth_test_3\": eval_results[\"eval\"][task_id][solution_id][\"base_details\"][2]==1 \n",
    "                    #    if len(eval_results[\"eval\"][task_id][solution_id][\"base_details\"])>2 else False,\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            print(task_id, solution_id)\n",
    "            print(exec_outputs[task_id][solution_id][\"base\"])\n",
    "            print(expected_output[task_id][\"base\"])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform final_outputs to pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df_final_outputs = pd.DataFrame(final_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df_final_outputs to pickle\n",
    "df_final_outputs.to_pickle(f\"{work_dir}/{dataset}_{gen_dir}_final_outputs.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepseek-coder-7b-instruct-v1.5_temp_0.8'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many result_unit_test_4 are not None across different src_idx\n",
    "result_unit_test_4_count = df_final_outputs.groupby('src_idx')['result_unit_test_4'].apply(lambda x: x.notnull().sum())\n",
    "# print the src_idx where result_unit_test_4_count > 0\n",
    "#print(result_unit_test_4_count[result_unit_test_4_count > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mbpp/737 check_str\n",
      "Mbpp/787 text_match_three\n",
      "Mbpp/794 text_starta_endb\n"
     ]
    }
   ],
   "source": [
    "from evalplus.data import get_mbpp_plus\n",
    "problems = get_mbpp_plus()\n",
    "MBPP_OUTPUT_NOT_NONE_TASKS = [\"check_str\", \"text_match_three\", \"text_starta_endb\"]\n",
    "for task_id in problems:\n",
    "    if problems[task_id][\"entry_point\"] in MBPP_OUTPUT_NOT_NONE_TASKS:\n",
    "        print(task_id, problems[task_id][\"entry_point\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False\n",
    "\n",
    "def utility(output_hyp, output_ref, src_idx, sample_idx):\n",
    "    if output_hyp is None or output_ref is None:\n",
    "        return False\n",
    "    if output_hyp.startswith(\"failed\"):\n",
    "        return False\n",
    "    exact_match = output_hyp == output_ref\n",
    "    if is_floats(output_hyp) or is_floats(output_ref):\n",
    "        if not exact_match:\n",
    "            exact_match = np.isclose(output_hyp, output_ref, atol=1e-6)\n",
    "    return exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['similar_elements',\n",
       " 'find_char_long',\n",
       " 'common_in_nested_lists',\n",
       " 'extract_singly',\n",
       " 'larg_nnum',\n",
       " 'intersection_array',\n",
       " 'find_dissimilar',\n",
       " 'Diff']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MBPP_OUTPUT_SET_EQ_TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "7\n",
      "111\n",
      "140\n",
      "232\n",
      "249\n",
      "579\n",
      "769\n"
     ]
    }
   ],
   "source": [
    "for task_id in problems:\n",
    "    if problems[task_id][\"entry_point\"] in MBPP_OUTPUT_SET_EQ_TASKS:\n",
    "        print(task_id.split(\"/\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "7\n",
      "111\n",
      "140\n",
      "232\n",
      "249\n",
      "579\n",
      "769\n"
     ]
    }
   ],
   "source": [
    "for task_id in problems:\n",
    "    if problems[task_id][\"entry_point\"] in MBPP_OUTPUT_SET_EQ_TASKS:\n",
    "        print(task_id.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737\n",
      "787\n",
      "794\n"
     ]
    }
   ],
   "source": [
    "for task_id in problems:\n",
    "    if problems[task_id][\"entry_point\"] in MBPP_OUTPUT_NOT_NONE_TASKS:\n",
    "        print(task_id.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n"
     ]
    }
   ],
   "source": [
    "for task_id in problems:\n",
    "    if problems[task_id][\"entry_point\"] == \"sum_div\":\n",
    "        print(task_id.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "MBPP_OUTPUT_SET_EQ_TASK_IDS = [2,7,111,140,232,249,579,769]\n",
    "MBPP_OUTPUT_NOT_NONE_TASK_IDS = [737,787,794]\n",
    "\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False\n",
    "\n",
    "def utility(hyp_ut, ref_ut, src_idx, dataset, inp=None, atol=0):\n",
    "    \n",
    "    if hyp_ut.startswith(\"failed:\") or ref_ut.startswith(\"failed:\"):\n",
    "        return False\n",
    "\n",
    "    exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    # ================================================ #\n",
    "    # ============== special oracles ================= #\n",
    "    if dataset == \"mbpp\":\n",
    "        if 164 == src_idx:  # Mbpp/164 special oracle\n",
    "            exact_match = exact_match or True\n",
    "        elif 295 == src_idx:  # Mbpp/295 special oracle\n",
    "            exact_match = exact_match or hyp_ut == 0 or ref_ut == 0\n",
    "        elif src_idx in MBPP_OUTPUT_SET_EQ_TASK_IDS:\n",
    "            exact_match = set(hyp_ut) == set(ref_ut)\n",
    "        elif src_idx in MBPP_OUTPUT_NOT_NONE_TASK_IDS:\n",
    "            # exp is True  if not None\n",
    "            #        False if None\n",
    "            if isinstance(hyp_ut, bool):\n",
    "                hyp_ut = hyp_ut is not None\n",
    "            if isinstance(ref_ut, bool):\n",
    "                ref_ut = ref_ut is not None\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    if dataset == \"humaneval\":\n",
    "        raise NotImplementedError # TODO: implement humaneval special entry points\n",
    "        if \"find_zero\" == entry_point:\n",
    "            hyp_ut = _poly(*inp, hyp_ut) <= atol\n",
    "            ref_ut = _poly(*inp, ref_ut) <= atol\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "    # ============== special oracles ================= #\n",
    "    # ================================================ #\n",
    "\n",
    "    if atol == 0 and (is_floats(ref_ut) or is_floats(hyp_ut)):\n",
    "        atol = 1e-6  # enforce atol for float comparison\n",
    "    if not exact_match and atol != 0:\n",
    "        # explicitly set rtol=1e-07\n",
    "        # to match `np.testing.assert_allclose`'s default values\n",
    "        exact_match =  np.allclose(hyp_ut, ref_ut, rtol=1e-07, atol=atol)\n",
    "    \n",
    "    return exact_match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

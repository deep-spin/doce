{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from warnings import warn\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from evalplus.data.utils import CACHE_DIR\n",
    "\n",
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "\n",
    "from evalplus.eval._special_oracle import (\n",
    "    MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    MBPP_OUTPUT_SET_EQ_TASKS,\n",
    "    _poly,\n",
    ")\n",
    "\n",
    "from evalplus.gen.util import trusted_exec\n",
    "\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False\n",
    "\n",
    "def ut_exact_match(\n",
    "    hyp_ut, \n",
    "    ref_ut, \n",
    "    entry_point, \n",
    "    dataset, \n",
    "    inp=None, \n",
    "    atol=0 # need to change this later\n",
    "    ):\n",
    "    exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    # ================================================ #\n",
    "    # ============== special oracles ================= #\n",
    "    if dataset == \"mbpp\":\n",
    "        if \"are_equivalent\" == entry_point:  # Mbpp/164 special oracle\n",
    "            exact_match = exact_match or True\n",
    "        elif \"sum_div\" == entry_point:  # Mbpp/295 special oracle\n",
    "            exact_match = exact_match or hyp_ut == 0 or ref_ut == 0\n",
    "        elif entry_point in MBPP_OUTPUT_SET_EQ_TASKS:\n",
    "            exact_match = set(hyp_ut) == set(ref_ut)\n",
    "        elif entry_point in MBPP_OUTPUT_NOT_NONE_TASKS:\n",
    "            # exp is True  if not None\n",
    "            #        False if None\n",
    "            if isinstance(hyp_ut, bool):\n",
    "                hyp_ut = hyp_ut is not None\n",
    "            if isinstance(ref_ut, bool):\n",
    "                ref_ut = ref_ut is not None\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "\n",
    "    if dataset == \"humaneval\":\n",
    "        if \"find_zero\" == entry_point:\n",
    "            hyp_ut = _poly(*inp, hyp_ut) <= atol\n",
    "            ref_ut = _poly(*inp, ref_ut) <= atol\n",
    "            exact_match = hyp_ut == ref_ut\n",
    "    # ============== special oracles ================= #\n",
    "    # ================================================ #\n",
    "\n",
    "    if atol == 0 and (is_floats(ref_ut) or is_floats(hyp_ut)):\n",
    "        atol = 1e-6  # enforce atol for float comparison\n",
    "    if not exact_match and atol != 0:\n",
    "        # explicitly set rtol=1e-07\n",
    "        # to match `np.testing.assert_allclose`'s default values\n",
    "        exact_match =  np.allclose(hyp_ut, ref_ut, rtol=1e-07, atol=atol)\n",
    "    \n",
    "    return int(exact_match)\n",
    "\n",
    "def get_groundtruth(problems, hashcode, tasks_only_output_not_none):\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{hashcode}.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        #print(f\"Load from ground-truth from {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    #print(\"Computing expected output...\")\n",
    "    tbegin = time.time()\n",
    "    expected_output = {}\n",
    "    for task_id, problem in problems.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"base_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = trusted_exec(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"plus_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "            output_not_none=problem[\"entry_point\"] in tasks_only_output_not_none,\n",
    "        )\n",
    "        expected_output[task_id] = oracle\n",
    "    #print(f\"Expected outputs computed in {time.time() - tbegin:.2f}s\")\n",
    "\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(expected_output, f)\n",
    "\n",
    "    return expected_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_exec(hyp_uts, ref_uts, entry_point, dataset, n_uts, inps=None, granular=False):\n",
    "    n_matches = 0\n",
    "    for i in range(n_uts):\n",
    "        # skip if either hyp_ut or ref_ut is not in the list\n",
    "        if i not in hyp_uts or i not in ref_uts:\n",
    "            continue\n",
    "        # if there's an error, we return 0\n",
    "        if type(hyp_uts[i]) == str and hyp_uts[i].startswith(\"failed:\"):\n",
    "            return 0 \n",
    "        if type(ref_uts[i]) == str and ref_uts[i].startswith(\"failed:\"):\n",
    "            return 0\n",
    "        # we start counting the number of matches\n",
    "        try:\n",
    "            n_matches += ut_exact_match(\n",
    "                hyp_uts[i], \n",
    "                ref_uts[i], \n",
    "                entry_point, \n",
    "                dataset, \n",
    "                inp=inps[i] if inps else None\n",
    "                )\n",
    "        except:\n",
    "            n_matches += 0\n",
    "        \n",
    "    if granular:\n",
    "        try:\n",
    "            return n_matches/ n_uts\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        return int(n_matches == n_uts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec_outputs loaded\n",
      "eval_results loaded\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs\"\n",
    "dataset = \"mbpp\"\n",
    "#gen_dir = \"deepseek-coder-33b-instruct_temp_0.8\"\n",
    "#gen_dir = \"deepseek-coder-7b-instruct-v1.5_temp_1.2\"\n",
    "#gen_dir = \"deepseek-coder-6.7b-instruct_temp_1.2\"\n",
    "gen_dir = \"code-llama-13b-instruct_temp_1.6\"\n",
    "#gen_dir = \"code-llama-7b-instruct_temp_1.6\"\n",
    "#debug_gen_dir = gen_dir + \"_debug1_not_change_positive\"\n",
    "debug_gen_dir = gen_dir + \"_debug1_sd-ut\"\n",
    "debug_3times_gen_dir = gen_dir + \"_debug3_sd-ut\"\n",
    "#_debug1_not_change_positive\n",
    "# load exec_outputs\n",
    "\n",
    "# load problems\n",
    "if dataset == \"mbpp\":\n",
    "    problems = get_mbpp_plus()\n",
    "    dataset_hash = get_mbpp_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        MBPP_OUTPUT_NOT_NONE_TASKS,\n",
    "    )\n",
    "elif dataset == \"humaneval\":\n",
    "    problems = get_human_eval_plus()\n",
    "    dataset_hash = get_human_eval_plus_hash()\n",
    "    expected_output = get_groundtruth(\n",
    "        problems,\n",
    "        dataset_hash,\n",
    "        []\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset\")\n",
    "\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/exec_outputs_v2.pkl\", \"rb\") as f:\n",
    "    exec_outputs = pickle.load(f)\n",
    "print(\"exec_outputs loaded\")\n",
    "\n",
    "# load exec_outputs\n",
    "#with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/exec_outputs_v2.pkl\", \"rb\") as f:\n",
    "#    exec_outputs_debug = pickle.load(f)\n",
    "#print(\"exec_outputs_debug loaded\")\n",
    "\n",
    "# load eval_results\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/eval_results.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "    \n",
    "#pop out [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]\n",
    "if dataset == \"mbpp\":\n",
    "    for task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        eval_results[\"eval\"].pop(task_id)\n",
    "print(\"eval_results loaded\")\n",
    "\n",
    "# load eval_results\n",
    "#with open(f\"{work_dir}/{dataset}/{debug_gen_dir}/eval_results.json\", \"r\") as f:\n",
    "#    ape_eval_results = json.load(f)\n",
    "#for task_id in ape_eval_results[\"eval\"]:\n",
    "#    ape_eval_results[\"eval\"][task_id] = sorted(ape_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "#print(\"ape_eval_results loaded\")\n",
    "\n",
    "# load eval_results\n",
    "#with open(f\"{work_dir}/{dataset}/{debug_3times_gen_dir}/eval_results.json\", \"r\") as f:\n",
    "#    ape_3times_eval_results = json.load(f)\n",
    "#for task_id in ape_3times_eval_results[\"eval\"]:\n",
    "#    ape_3times_eval_results[\"eval\"][task_id] = sorted(ape_3times_eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs loaded\n",
      "reviewer_logprobs loaded\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/logprobs.pkl\", \"rb\") as f:\n",
    "    logprobs = pickle.load(f)\n",
    "print(\"logprobs loaded\")\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/reviewer_logprobs.pkl\", \"rb\") as f:\n",
    "    reviewer_logprobs = pickle.load(f)\n",
    "print(\"reviewer_logprobs loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f\"{work_dir}/{dataset}/{gen_dir}/llm_score_yn.pkl\", \"rb\") as f:\n",
    "#    llm_score_yn = pickle.load(f)\n",
    "#print(\"llm_score_yn loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/errors.pkl\", \"rb\") as f:\n",
    "    errors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor task_id in eval_results[\"eval\"]:\\n    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\\nfor task_id in errors:\\n    if task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\\n        continue\\n    for i in range(len(errors[task_id])):\\n        if errors[task_id][i][\"base\"][\"status\"] == \"pass\":\\n            assert len(eval_results[\"eval\"][task_id][i][\"base_details\"])> 0\\n            assert eval_results[\"eval\"][task_id][i][\"base_details\"][0] == 1\\n        else:\\n            if len(eval_results[\"eval\"][task_id][i][\"base_details\"]) > 0:\\n                assert eval_results[\"eval\"][task_id][i][\"base_details\"][0] == 0, (task_id, i)\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "for task_id in errors:\n",
    "    if task_id in [\"Mbpp/6\", \"Mbpp/7\", \"Mbpp/8\", \"Mbpp/9\"]:\n",
    "        continue\n",
    "    for i in range(len(errors[task_id])):\n",
    "        if errors[task_id][i][\"base\"][\"status\"] == \"pass\":\n",
    "            assert len(eval_results[\"eval\"][task_id][i][\"base_details\"])> 0\n",
    "            assert eval_results[\"eval\"][task_id][i][\"base_details\"][0] == 1\n",
    "        else:\n",
    "            if len(eval_results[\"eval\"][task_id][i][\"base_details\"]) > 0:\n",
    "                assert eval_results[\"eval\"][task_id][i][\"base_details\"][0] == 0, (task_id, i)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "code_scores = load_jsonl(f\"{work_dir}/{dataset}/{gen_dir}/code_score.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774390243902439\n",
      "0.6646341463414634\n"
     ]
    }
   ],
   "source": [
    "#for e in eval_results[\"eval\"][\"HumanEval/156\"]:\n",
    "#    print(e[\"solution_id\"])\n",
    "#    print(e[\"base_status\"])\n",
    "#    print(e[\"plus_status\"])\n",
    "\n",
    "include_filtering = True\n",
    "max_id = 50\n",
    "min_id = 0\n",
    "code_score_dict = {}\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    code_score_dict[task_id] = [0.0] * len(eval_results[\"eval\"][task_id])\n",
    "for elem in code_scores:\n",
    "    for id in elem[\"generated_code_solution_ids\"]:\n",
    "        p_name = elem[\"task_id\"].replace(\"/\", \"_\")\n",
    "        if id < min_id or id >= max_id:\n",
    "            continue\n",
    "        if include_filtering:\n",
    "            if errors[elem[\"task_id\"]][id][\"base\"][\"status\"] != \"pass\":\n",
    "                continue\n",
    "        #code_score_dict[elem[\"task_id\"]][id] = np.exp(np.mean(logprobs[p_name][id]))\n",
    "        code_score_dict[elem[\"task_id\"]][id] = np.exp(np.log(elem[\"predict_score\"]))\n",
    "        #code_score_dict[elem[\"task_id\"]][id] = np.exp(np.log(elem[\"predict_score\"]) + np.mean(logprobs[p_name][id]))\n",
    "\n",
    "# for each task_id, for each solution_id, get the argmax of the code_score_dict[task_id]\n",
    "argmax_code_scores = {}\n",
    "for task_id in code_score_dict:\n",
    "    argmax_code_scores[task_id] = np.argmax(code_score_dict[task_id])\n",
    "base_correct = []\n",
    "plus_correct = []\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    argmax_code_score = argmax_code_scores[task_id]\n",
    "    assert argmax_code_score == int(eval_results[\"eval\"][task_id][argmax_code_score][\"solution_id\"])\n",
    "    base_status = eval_results[\"eval\"][task_id][argmax_code_score][\"base_status\"]\n",
    "    plus_status = eval_results[\"eval\"][task_id][argmax_code_score][\"plus_status\"]\n",
    "    base_correct.append(base_status == \"pass\")\n",
    "    plus_correct.append(plus_status == base_status == \"pass\")\n",
    "print(np.mean(base_correct))\n",
    "print(np.mean(plus_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-llama-7b-humaneval-codescore-logprob\n",
      "0.33079268292682923\n",
      "0.3018292682926829\n",
      "code-llama-7b-filtering-humaneval-codescore-logprob\n",
      "0.711890243902439\n",
      "0.6097560975609756\n",
      "code-llama-13b-humaneval-codescore-logprob\n",
      "0.35823170731707316\n",
      "0.31707317073170727\n",
      "code-llama-13b-filtering-humaneval-codescore-logprob\n",
      "0.7667682926829269\n",
      "0.6570121951219512\n",
      "deepseek-humaneval-codescore-logprob\n",
      "0.701219512195122\n",
      "0.635670731707317\n",
      "deepseek-filtering-humaneval-codescore-logprob\n",
      "0.8765243902439024\n",
      "0.7835365853658538\n"
     ]
    }
   ],
   "source": [
    "print(\"code-llama-7b-humaneval-codescore-logprob\")\n",
    "print(np.mean([0.35365853658536583, 0.3353658536585366, 0.35365853658536583, 0.2804878048780488]))\n",
    "print(np.mean([0.31097560975609756, 0.3170731707317073, 0.31097560975609756, 0.2682926829268293]))\n",
    "\n",
    "print(\"code-llama-7b-filtering-humaneval-codescore-logprob\")\n",
    "print(np.mean([0.7012195121951219, 0.725609756097561, 0.7195121951219512, 0.7012195121951219]))\n",
    "print(np.mean([0.5975609756097561, 0.6097560975609756, 0.6097560975609756, 0.6219512195121951]))\n",
    "\n",
    "print(\"code-llama-13b-humaneval-codescore-logprob\")\n",
    "print(np.mean([0.34146341463414637, 0.32926829268292684, 0.34146341463414637, 0.42073170731707316]))\n",
    "print(np.mean([0.31097560975609756, 0.2865853658536585, 0.29878048780487804, 0.3719512195121951]))\n",
    "\n",
    "print(\"code-llama-13b-filtering-humaneval-codescore-logprob\")\n",
    "print(np.mean([0.774390243902439, 0.75, 0.774390243902439, 0.7682926829268293]))\n",
    "print(np.mean([0.6646341463414634, 0.6402439024390244, 0.6585365853658537, 0.6646341463414634]))\n",
    "\n",
    "print(\"deepseek-humaneval-codescore-logprob\")\n",
    "print(np.mean([0.6707317073170732, 0.7012195121951219, 0.7439024390243902, 0.6890243902439024]))\n",
    "print(np.mean([0.6341463414634146, 0.6341463414634146, 0.6585365853658537, 0.6158536585365854]))\n",
    "\n",
    "print(\"deepseek-filtering-humaneval-codescore-logprob\")\n",
    "print(np.mean([0.8841463414634146, 0.8597560975609756, 0.8841463414634146, 0.8780487804878049]))\n",
    "print(np.mean([0.8048780487804879, 0.774390243902439, 0.7865853658536586, 0.7682926829268293]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7336785268806619\n",
      "0.6258826617542953\n"
     ]
    }
   ],
   "source": [
    "max_id = 50\n",
    "min_id = 0\n",
    "\n",
    "\n",
    "#baseline for filtering\n",
    "\n",
    "filtering_dict = {}\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    filtering_dict[task_id] = [0.0] * len(eval_results[\"eval\"][task_id])\n",
    "    \n",
    "#for elem in code_scores:\n",
    "#    for id in elem[\"generated_code_solution_ids\"]:\n",
    "#        if id < min_id or id >= max_id:\n",
    "#            continue\n",
    "#        if errors[elem[\"task_id\"]][id][\"base\"][\"status\"] == \"pass\":\n",
    "#            filtering_dict[elem[\"task_id\"]][id] = 1\n",
    "\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    for id in range(len(eval_results[\"eval\"][task_id])):\n",
    "        if id < min_id or id >= max_id:\n",
    "            continue\n",
    "        if errors[task_id][id][\"base\"][\"status\"] == \"pass\":\n",
    "        #if len(eval_results[\"eval\"][task_id][id][\"base_details\"]) > 0:\n",
    "        #    if eval_results[\"eval\"][task_id][id][\"base_details\"][0] == 1:\n",
    "            filtering_dict[task_id][id] = 1\n",
    "\n",
    "base_true_positive_rate = {}\n",
    "plus_true_positive_rate = {}\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    base_true_positive_rate[task_id] = [0.0] * len(eval_results[\"eval\"][task_id])\n",
    "    plus_true_positive_rate[task_id] = [0.0] * len(eval_results[\"eval\"][task_id])\n",
    "    \n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    for id in range(len(eval_results[\"eval\"][task_id])):\n",
    "        if id < min_id or id >= max_id:\n",
    "            continue\n",
    "        if errors[task_id][id][\"base\"][\"status\"] == \"pass\" \\\n",
    "            and eval_results[\"eval\"][task_id][id][\"base_status\"] == \"pass\":\n",
    "        #if filtering_dict[task_id][id] == 1 and eval_results[\"eval\"][task_id][id][\"base_status\"] == \"pass\":\n",
    "            base_true_positive_rate[task_id][id] = 1\n",
    "            if eval_results[\"eval\"][task_id][id][\"plus_status\"] == \"pass\":\n",
    "                plus_true_positive_rate[task_id][id] = 1\n",
    "\n",
    "base_final_scores = []\n",
    "plus_final_scores = []\n",
    "for task_id in base_true_positive_rate:\n",
    "    try:\n",
    "        base_final_scores.append(sum(base_true_positive_rate[task_id])/sum(filtering_dict[task_id]))\n",
    "    except:\n",
    "        base_final_scores.append(0)\n",
    "    try:\n",
    "        plus_final_scores.append(sum(plus_true_positive_rate[task_id])/sum(filtering_dict[task_id]))\n",
    "    except:\n",
    "        plus_final_scores.append(0)\n",
    "\n",
    "print(np.mean(base_final_scores))\n",
    "print(np.mean(plus_final_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-llama-7b-humaneval\n",
      "0.7000880421349245\n",
      "0.5969683117158375\n",
      "code-llama-13b-humaneval\n",
      "0.7535809322663974\n",
      "0.6341903959213012\n",
      "deepseek-humaneval\n",
      "0.8836987323267176\n",
      "0.7873245667777552\n"
     ]
    }
   ],
   "source": [
    "#humaneval filtering baseline\n",
    "print(\"code-llama-7b-humaneval\")\n",
    "print(np.mean([0.693160186459372, 0.6997735981934243, 0.72117062444169, 0.6862477594452113]))\n",
    "print(np.mean([0.5908040389033249, 0.6011116883717889, 0.6137140602360607, 0.5822434593521757]))\n",
    "\n",
    "print(\"code-llama-13b-humaneval\")\n",
    "print(np.mean([0.7662873506194061, 0.7514377984534123, 0.7629200531121093, 0.7336785268806619]))\n",
    "print(np.mean([0.643287595977476, 0.631051116450025, 0.6365402095034088, 0.6258826617542953]))\n",
    "\n",
    "print(\"deepseek-humaneval\")\n",
    "print(np.mean([0.8820033571614998, 0.8883587387079996, 0.881986977881884, 0.882445855555487]))\n",
    "print(np.mean([0.7827359898849421, 0.7918541518655567, 0.7854197999920142, 0.7892883253685077]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-llama-7b-mbpp\n",
      "0.7741791372118685\n",
      "0.6081458614625987\n",
      "code-llama-13b-mbpp\n",
      "0.819114905729112\n",
      "0.6469159563120278\n",
      "deepseek-mbpp\n",
      "0.8736574966406614\n",
      "0.726060766929703\n"
     ]
    }
   ],
   "source": [
    "#mbpp filtering baseline\n",
    "print(\"code-llama-7b-mbpp\")\n",
    "print(np.mean([0.7805309287120359, 0.7724022207392057, 0.7731006556120122, 0.7706827437842202]))\n",
    "print(np.mean([0.6104730168236094, 0.6064812230767918, 0.6092237984535773, 0.6064054074964162]))\n",
    "\n",
    "print(\"code-llama-13b-mbpp\")\n",
    "print(np.mean([0.8138750771906944, 0.8217007921648252, 0.8200892856053096, 0.820794467955619]))\n",
    "print(np.mean([0.6479763222924249, 0.6527857093397472, 0.6456394161083959, 0.6412623775075433]))\n",
    "\n",
    "print(\"deepseek-mbpp\")\n",
    "print(np.mean([0.8758127616516805, 0.8774858296693084, 0.8713415329802104, 0.8699898622614464]))\n",
    "print(np.mean([0.7300904709039442, 0.7236647749261638, 0.7207821623236315, 0.7297056595650726]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830379746835443\n",
      "0.6658227848101266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "include_filtering = True\n",
    "min_id = 0\n",
    "max_id = min_id + 50\n",
    "base_correct = []\n",
    "plus_correct = []\n",
    "with open(f\"{work_dir}/{dataset}/{gen_dir}/errors.pkl\", \"rb\") as f:\n",
    "    errors = pickle.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    p_name = task_id.replace(\"/\", \"_\")\n",
    "    mean_logprobs = []\n",
    "    for i in range(len(eval_results[\"eval\"][task_id])):\n",
    "        if i < min_id or i >= max_id:\n",
    "            continue\n",
    "        assert i == int(eval_results[\"eval\"][task_id][i][\"solution_id\"])\n",
    "        if include_filtering:\n",
    "            passes = errors[task_id][i][\"base\"][\"status\"] == \"pass\"\n",
    "            if not passes:\n",
    "                mean_logprobs.append(0)\n",
    "            else:\n",
    "                #mean_logprobs.append(np.exp(np.mean(reviewer_logprobs[p_name][i])))\n",
    "                #mean_logprobs.append(np.exp(np.mean(logprobs[p_name][i])))\n",
    "                mean_logprobs.append(np.exp(np.mean(reviewer_logprobs[p_name][i])) + np.exp(np.mean(logprobs[p_name][i])))\n",
    "        else:\n",
    "            #mean_logprobs.append(np.exp(np.mean(reviewer_logprobs[p_name][i])))\n",
    "            #mean_logprobs.append(np.exp(np.mean(logprobs[p_name][i])))\n",
    "            mean_logprobs.append(np.exp(np.mean(reviewer_logprobs[p_name][i])) + np.exp(np.mean(logprobs[p_name][i])))\n",
    "    # get the argmax of mean_logprobs\n",
    "    argmax_logprobs = np.argmax(mean_logprobs) + min_id\n",
    "    # see if the argmax_logprobs is in the list of solution_ids\n",
    "    assert argmax_logprobs == int(eval_results[\"eval\"][task_id][argmax_logprobs][\"solution_id\"])\n",
    "    base_status = eval_results[\"eval\"][task_id][argmax_logprobs][\"base_status\"]\n",
    "    plus_status = eval_results[\"eval\"][task_id][argmax_logprobs][\"plus_status\"]\n",
    "    base_correct.append(base_status == \"pass\")\n",
    "    plus_correct.append(plus_status == base_status == \"pass\")\n",
    "print(np.mean(base_correct))\n",
    "print(np.mean(plus_correct))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_base\n",
      "57.91139240506329\n",
      "logprob_plus\n",
      "47.151898734177216\n",
      "coder_reviewer_logprob_base\n",
      "60.379746835443036\n",
      "coder_reviewer_logprob_plus\n",
      "49.493670886075954\n",
      "filtering_logprob_base\n",
      "82.78481012658227\n",
      "filtering_logprob_plus\n",
      "64.05063291139241\n",
      "filtering_base\n",
      "81.9114905729112\n",
      "filtering_plus\n",
      "64.69159563120279\n",
      "filtering_coder_reviewer_logprob_base\n",
      "82.72151898734177\n",
      "filtering_coder_reviewer_logprob_plus\n",
      "64.99999999999999\n"
     ]
    }
   ],
   "source": [
    "### cl13b-mbpp\n",
    "logprob_base = [0.5670886075949367, 0.589873417721519, 0.5924050632911393, 0.5670886075949367]\n",
    "logprob_plus = [0.4810126582278481, 0.46582278481012657, 0.4835443037974684, 0.45569620253164556]\n",
    "coder_reviewer_logprob_base = [0.6050632911392405, 0.6151898734177215, 0.6, 0.5949367088607594]\n",
    "coder_reviewer_logprob_plus = [0.5063291139240507, 0.5063291139240507, 0.49873417721518987, 0.46835443037974683]\n",
    "filtering_logprob_base = [0.830379746835443, 0.8253164556962025, 0.8278481012658228, 0.8278481012658228]\n",
    "filtering_logprob_plus = [0.6506329113924051, 0.640506329113924, 0.6455696202531646, 0.6253164556962025]\n",
    "filtering_base = [0.8138750771906944, 0.8217007921648252, 0.8200892856053096, 0.820794467955619]\n",
    "filtering_plus = [0.6479763222924249, 0.6527857093397472, 0.6456394161083959, 0.6412623775075433]\n",
    "filtering_coder_reviewer_logprob_base = [0.830379746835443, 0.830379746835443, 0.8227848101265823, 0.8253164556962025]\n",
    "filtering_coder_reviewer_logprob_plus = [0.6658227848101266, 0.6632911392405063, 0.6481012658227848, 0.6227848101265823]\n",
    "# for each list, compute average and times 100\n",
    "list_names = [\"logprob_base\", \"logprob_plus\", \"coder_reviewer_logprob_base\", \"coder_reviewer_logprob_plus\", \"filtering_logprob_base\", \"filtering_logprob_plus\", \"filtering_base\", \"filtering_plus\", \"filtering_coder_reviewer_logprob_base\", \"filtering_coder_reviewer_logprob_plus\"]\n",
    "lists = [logprob_base, logprob_plus, coder_reviewer_logprob_base, coder_reviewer_logprob_plus, filtering_logprob_base, filtering_logprob_plus, filtering_base, filtering_plus, filtering_coder_reviewer_logprob_base, filtering_coder_reviewer_logprob_plus]\n",
    "\n",
    "for name, lst in zip(list_names, lists):\n",
    "    # print the name of the list\n",
    "    print(name)\n",
    "    print(np.mean(lst) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_base\n",
      "53.963414634146346\n",
      "logprob_plus\n",
      "45.579268292682926\n",
      "coder_reviewer_logprob_base\n",
      "51.06707317073172\n",
      "coder_reviewer_logprob_plus\n",
      "43.75\n",
      "filtering_logprob_base\n",
      "77.4390243902439\n",
      "filtering_logprob_plus\n",
      "65.70121951219512\n",
      "filtering_base\n",
      "75.35809322663974\n",
      "filtering_plus\n",
      "63.41903959213012\n",
      "filtering_coder_reviewer_logprob_base\n",
      "76.82926829268293\n",
      "filtering_coder_reviewer_logprob_plus\n",
      "65.70121951219512\n"
     ]
    }
   ],
   "source": [
    "### cl13b-humaneval\n",
    "logprob_base = [0.5548780487804879, 0.5304878048780488, 0.5426829268292683, 0.5304878048780488]\n",
    "logprob_plus = [0.4817073170731707, 0.4451219512195122, 0.4695121951219512, 0.4268292682926829]\n",
    "coder_reviewer_logprob_base = [0.5182926829268293, 0.5060975609756098, 0.49390243902439024, 0.524390243902439]\n",
    "coder_reviewer_logprob_plus = [0.4634146341463415, 0.4268292682926829, 0.4329268292682927, 0.4268292682926829]\n",
    "filtering_logprob_base = [0.8048780487804879, 0.75, 0.7926829268292683, 0.75]\n",
    "filtering_logprob_plus = [0.6829268292682927, 0.6280487804878049, 0.6829268292682927, 0.6341463414634146]\n",
    "filtering_base = [0.7662873506194061, 0.7514377984534123, 0.7629200531121093, 0.7336785268806619]\n",
    "filtering_plus = [0.643287595977476, 0.631051116450025, 0.6365402095034088, 0.6258826617542953]\n",
    "filtering_coder_reviewer_logprob_base = [0.774390243902439, 0.7378048780487805, 0.7987804878048781, 0.7621951219512195]\n",
    "filtering_coder_reviewer_logprob_plus = [0.6646341463414634, 0.6280487804878049, 0.6890243902439024, 0.6463414634146342]\n",
    "# for each list, compute average and times 100\n",
    "list_names = [\"logprob_base\", \"logprob_plus\", \"coder_reviewer_logprob_base\", \"coder_reviewer_logprob_plus\", \"filtering_logprob_base\", \"filtering_logprob_plus\", \"filtering_base\", \"filtering_plus\", \"filtering_coder_reviewer_logprob_base\", \"filtering_coder_reviewer_logprob_plus\"]\n",
    "lists = [logprob_base, logprob_plus, coder_reviewer_logprob_base, coder_reviewer_logprob_plus, filtering_logprob_base, filtering_logprob_plus, filtering_base, filtering_plus, filtering_coder_reviewer_logprob_base, filtering_coder_reviewer_logprob_plus]\n",
    "\n",
    "for name, lst in zip(list_names, lists):\n",
    "    # print the name of the list\n",
    "    print(name)\n",
    "    print(np.mean(lst) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_base\n",
      "81.40243902439023\n",
      "logprob_plus\n",
      "71.79878048780488\n",
      "coder_reviewer_logprob_base\n",
      "78.65853658536585\n",
      "coder_reviewer_logprob_plus\n",
      "70.27439024390243\n",
      "filtering_logprob_base\n",
      "90.54878048780488\n",
      "filtering_logprob_plus\n",
      "79.11585365853658\n",
      "filtering_base\n",
      "88.36987323267176\n",
      "filtering_plus\n",
      "78.73245667777552\n",
      "filtering_coder_reviewer_logprob_base\n",
      "89.48170731707317\n",
      "filtering_coder_reviewer_logprob_plus\n",
      "79.57317073170731\n"
     ]
    }
   ],
   "source": [
    "### deepseek-humaneval\n",
    "logprob_base = [0.8170731707317073, 0.7987804878048781, 0.8414634146341463, 0.7987804878048781]\n",
    "logprob_plus = [0.7134146341463414, 0.7134146341463414, 0.75, 0.6951219512195121]\n",
    "coder_reviewer_logprob_base = [0.7865853658536586, 0.7804878048780488, 0.8109756097560976, 0.7682926829268293]\n",
    "coder_reviewer_logprob_plus = [0.7012195121951219, 0.7012195121951219, 0.7317073170731707, 0.676829268292683]\n",
    "filtering_logprob_base = [0.9146341463414634, 0.8963414634146342, 0.9085365853658537, 0.9024390243902439]\n",
    "filtering_logprob_plus = [0.7926829268292683, 0.7865853658536586, 0.7987804878048781, 0.7865853658536586]\n",
    "filtering_base = [0.8820033571614998, 0.8883587387079996, 0.881986977881884, 0.882445855555487]\n",
    "filtering_plus = [0.7827359898849421, 0.7918541518655567, 0.7854197999920142, 0.7892883253685077]\n",
    "filtering_coder_reviewer_logprob_base = [0.8963414634146342, 0.8963414634146342, 0.9024390243902439, 0.8841463414634146]\n",
    "filtering_coder_reviewer_logprob_plus = [0.7926829268292683, 0.8109756097560976, 0.7987804878048781, 0.7804878048780488]\n",
    "# for each list, compute average and times 100\n",
    "list_names = [\"logprob_base\", \"logprob_plus\", \"coder_reviewer_logprob_base\", \"coder_reviewer_logprob_plus\", \"filtering_logprob_base\", \"filtering_logprob_plus\", \"filtering_base\", \"filtering_plus\", \"filtering_coder_reviewer_logprob_base\", \"filtering_coder_reviewer_logprob_plus\"]\n",
    "lists = [logprob_base, logprob_plus, coder_reviewer_logprob_base, coder_reviewer_logprob_plus, filtering_logprob_base, filtering_logprob_plus, filtering_base, filtering_plus, filtering_coder_reviewer_logprob_base, filtering_coder_reviewer_logprob_plus]\n",
    "\n",
    "for name, lst in zip(list_names, lists):\n",
    "    # print the name of the list\n",
    "    print(name)\n",
    "    print(np.mean(lst) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_base\n",
      "70.0632911392405\n",
      "logprob_plus\n",
      "58.924050632911396\n",
      "coder_reviewer_logprob_base\n",
      "76.13924050632912\n",
      "coder_reviewer_logprob_plus\n",
      "63.98734177215191\n",
      "filtering_logprob_base\n",
      "88.54430379746836\n",
      "filtering_logprob_plus\n",
      "72.84810126582279\n",
      "filtering_base\n",
      "87.36574966406614\n",
      "filtering_plus\n",
      "72.6060766929703\n",
      "filtering_coder_reviewer_logprob_base\n",
      "88.22784810126582\n",
      "filtering_coder_reviewer_logprob_plus\n",
      "72.78481012658227\n"
     ]
    }
   ],
   "source": [
    "### deepseek-mbpp\n",
    "logprob_base = [0.6708860759493671, 0.6886075949367089, 0.7189873417721518, 0.7240506329113924]\n",
    "logprob_plus = [0.5772151898734177, 0.5873417721518988, 0.5949367088607594, 0.5974683544303797]\n",
    "coder_reviewer_logprob_base = [0.7493670886075949, 0.769620253164557, 0.7645569620253164, 0.7620253164556962]\n",
    "coder_reviewer_logprob_plus = [0.6329113924050633, 0.6531645569620254, 0.6227848101265823, 0.6506329113924051]\n",
    "filtering_logprob_base = [0.8759493670886076, 0.8936708860759494, 0.8911392405063291, 0.8810126582278481]\n",
    "filtering_logprob_plus = [0.7316455696202532, 0.7240506329113924, 0.7240506329113924, 0.7341772151898734]\n",
    "filtering_base = [0.8758127616516805, 0.8774858296693084, 0.8713415329802104, 0.8699898622614464]\n",
    "filtering_plus = [0.7300904709039442, 0.7236647749261638, 0.7207821623236315, 0.7297056595650726]\n",
    "filtering_coder_reviewer_logprob_base = [0.8835443037974684, 0.8936708860759494, 0.8784810126582279, 0.8734177215189873]\n",
    "filtering_coder_reviewer_logprob_plus = [0.739240506329114, 0.7291139240506329, 0.7113924050632912, 0.7316455696202532]\n",
    "# for each list, compute average and times 100\n",
    "list_names = [\"logprob_base\", \"logprob_plus\", \"coder_reviewer_logprob_base\", \"coder_reviewer_logprob_plus\", \"filtering_logprob_base\", \"filtering_logprob_plus\", \"filtering_base\", \"filtering_plus\", \"filtering_coder_reviewer_logprob_base\", \"filtering_coder_reviewer_logprob_plus\"]\n",
    "lists = [logprob_base, logprob_plus, coder_reviewer_logprob_base, coder_reviewer_logprob_plus, filtering_logprob_base, filtering_logprob_plus, filtering_base, filtering_plus, filtering_coder_reviewer_logprob_base, filtering_coder_reviewer_logprob_plus]\n",
    "\n",
    "for name, lst in zip(list_names, lists):\n",
    "    # print the name of the list\n",
    "    print(name)\n",
    "    print(np.mean(lst) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_base\n",
      "45.1219512195122\n",
      "logprob_plus\n",
      "38.71951219512194\n",
      "coder_reviewer_logprob_base\n",
      "46.03658536585366\n",
      "coder_reviewer_logprob_plus\n",
      "40.548780487804876\n",
      "filtering_logprob_base\n",
      "71.03658536585365\n",
      "filtering_logprob_plus\n",
      "60.36585365853659\n",
      "filtering_base\n",
      "70.00880421349245\n",
      "filtering_plus\n",
      "59.69683117158375\n",
      "filtering_coder_reviewer_logprob_base\n",
      "71.34146341463415\n",
      "filtering_coder_reviewer_logprob_plus\n",
      "61.28048780487805\n"
     ]
    }
   ],
   "source": [
    "### code-llama-7b-humaneval\n",
    "logprob_base = [0.43902439024390244, 0.4146341463414634, 0.4573170731707317, 0.49390243902439024]\n",
    "logprob_plus = [0.3780487804878049, 0.36585365853658536, 0.38414634146341464, 0.42073170731707316]\n",
    "coder_reviewer_logprob_base = [0.4573170731707317, 0.4451219512195122, 0.4634146341463415, 0.47560975609756095]\n",
    "coder_reviewer_logprob_plus = [0.40853658536585363, 0.4024390243902439, 0.4024390243902439, 0.40853658536585363]\n",
    "filtering_logprob_base = [0.6951219512195121, 0.7073170731707317, 0.725609756097561, 0.7134146341463414]\n",
    "filtering_logprob_plus = [0.5975609756097561, 0.6097560975609756, 0.6036585365853658, 0.6036585365853658]\n",
    "filtering_base = [0.693160186459372, 0.6997735981934243, 0.72117062444169, 0.6862477594452113]\n",
    "filtering_plus = [0.5908040389033249, 0.6011116883717889, 0.6137140602360607, 0.5822434593521757]\n",
    "filtering_coder_reviewer_logprob_base = [0.7012195121951219, 0.6951219512195121, 0.7439024390243902, 0.7134146341463414]\n",
    "filtering_coder_reviewer_logprob_plus = [0.6097560975609756, 0.5975609756097561, 0.6402439024390244, 0.6036585365853658]\n",
    "# for each list, compute average and times 100\n",
    "list_names = [\"logprob_base\", \"logprob_plus\", \"coder_reviewer_logprob_base\", \"coder_reviewer_logprob_plus\", \"filtering_logprob_base\", \"filtering_logprob_plus\", \"filtering_base\", \"filtering_plus\", \"filtering_coder_reviewer_logprob_base\", \"filtering_coder_reviewer_logprob_plus\"]\n",
    "lists = [logprob_base, logprob_plus, coder_reviewer_logprob_base, coder_reviewer_logprob_plus, filtering_logprob_base, filtering_logprob_plus, filtering_base, filtering_plus, filtering_coder_reviewer_logprob_base, filtering_coder_reviewer_logprob_plus]\n",
    "\n",
    "for name, lst in zip(list_names, lists):\n",
    "    # print the name of the list\n",
    "    print(name)\n",
    "    print(np.mean(lst) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_base\n",
      "53.48101265822785\n",
      "logprob_plus\n",
      "42.46835443037975\n",
      "coder_reviewer_logprob_base\n",
      "53.101265822784804\n",
      "coder_reviewer_logprob_plus\n",
      "43.101265822784804\n",
      "filtering_logprob_base\n",
      "78.10126582278481\n",
      "filtering_logprob_plus\n",
      "59.683544303797476\n",
      "filtering_base\n",
      "77.41791372118685\n",
      "filtering_plus\n",
      "60.81458614625988\n",
      "filtering_coder_reviewer_logprob_base\n",
      "78.48101265822784\n",
      "filtering_coder_reviewer_logprob_plus\n",
      "60.18987341772153\n"
     ]
    }
   ],
   "source": [
    "### code-llama-7b-mbpp\n",
    "logprob_base = [0.5164556962025316, 0.5164556962025316, 0.5594936708860759, 0.5468354430379747]\n",
    "logprob_plus = [0.4151898734177215, 0.4177215189873418, 0.44050632911392407, 0.4253164556962025]\n",
    "coder_reviewer_logprob_base = [0.5139240506329114, 0.5164556962025316, 0.5392405063291139, 0.5544303797468354]\n",
    "coder_reviewer_logprob_plus = [0.4177215189873418, 0.4151898734177215, 0.4430379746835443, 0.4481012658227848]\n",
    "filtering_logprob_base = [0.7949367088607595, 0.7822784810126582, 0.7746835443037975, 0.7721518987341772]\n",
    "filtering_logprob_plus = [0.6050632911392405, 0.5848101265822785, 0.6, 0.5974683544303797]\n",
    "filtering_base = [0.7805309287120359, 0.7724022207392057, 0.7731006556120122, 0.7706827437842202]\n",
    "filtering_plus = [0.6104730168236094, 0.6064812230767918, 0.6092237984535773, 0.6064054074964162]\n",
    "filtering_coder_reviewer_logprob_base = [0.7873417721518987, 0.779746835443038, 0.789873417721519, 0.7822784810126582]\n",
    "filtering_coder_reviewer_logprob_plus = [0.6075949367088608, 0.589873417721519, 0.610126582278481, 0.6]\n",
    "# for each list, compute average and times 100\n",
    "list_names = [\"logprob_base\", \"logprob_plus\", \"coder_reviewer_logprob_base\", \"coder_reviewer_logprob_plus\", \"filtering_logprob_base\", \"filtering_logprob_plus\", \"filtering_base\", \"filtering_plus\", \"filtering_coder_reviewer_logprob_base\", \"filtering_coder_reviewer_logprob_plus\"]\n",
    "lists = [logprob_base, logprob_plus, coder_reviewer_logprob_base, coder_reviewer_logprob_plus, filtering_logprob_base, filtering_logprob_plus, filtering_base, filtering_plus, filtering_coder_reviewer_logprob_base, filtering_coder_reviewer_logprob_plus]\n",
    "\n",
    "for name, lst in zip(list_names, lists):\n",
    "    # print the name of the list\n",
    "    print(name)\n",
    "    print(np.mean(lst) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = \"Mbpp/102\"\n",
    "for solution in eval_results[\"eval\"][task_id]:\n",
    "    solution_id = int(solution[\"solution_id\"])\n",
    "    p_name = task_id.replace(\"/\", \"_\")\n",
    "    pos_score, neg_score = llm_score_yn[p_name][solution_id][\"pos\"], llm_score_yn[p_name][solution_id][\"neg\"]\n",
    "    print(1/(np.exp(neg_score-pos_score)+1))\n",
    "    print(eval_results[\"eval\"][task_id][0][\"base_status\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46835443037974683\n",
      "0.41012658227848103\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "base_correct = []\n",
    "plus_correct = []\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    p_name = task_id.replace(\"/\", \"_\")\n",
    "    mean_logprobs = []\n",
    "    llm_scores = []\n",
    "    for i in range(len(eval_results[\"eval\"][task_id])):\n",
    "        assert i == int(eval_results[\"eval\"][task_id][i][\"solution_id\"])\n",
    "        pos_score, neg_score = llm_score_yn[p_name][i][\"pos\"], llm_score_yn[p_name][i][\"neg\"]\n",
    "        llm_score = - np.log( np.exp( neg_score - pos_score ) + 1)\n",
    "        llm_scores.append(llm_score)\n",
    "        mean_logprobs.append(np.mean(logprobs[p_name][i]))\n",
    "    # get the argmax of mean_logprobs\n",
    "    argmax_llm = np.argmax(np.array(llm_scores))\n",
    "    # see if the argmax_logprobs is in the list of solution_ids\n",
    "    base_status = eval_results[\"eval\"][task_id][argmax_llm][\"base_status\"]\n",
    "    plus_status = eval_results[\"eval\"][task_id][argmax_llm][\"plus_status\"]\n",
    "    base_correct.append(base_status == \"pass\")\n",
    "    plus_correct.append(plus_status == base_status == \"pass\")\n",
    "print(np.mean(base_correct))\n",
    "print(np.mean(plus_correct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.629206418991089"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_score_yn[p_name][i][\"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/mbpp\"\n",
    "#model_name = \"deepseek-coder-6.7b-instruct_temp_1.2\"\n",
    "model_name = \"code-llama-13b-instruct_temp_1.6\"\n",
    "debug_info = \"_debug3_sd-ut\"\n",
    "gen_dir = model_name + debug_info + \"_b4process\"\n",
    "original_dir = model_name + \"_debug2_sd-ut\"\n",
    "\n",
    "with open(f\"{work_dir}/{original_dir}/eval_results.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)\n",
    "for task_id in eval_results[\"eval\"]:\n",
    "    eval_results[\"eval\"][task_id] = sorted(eval_results[\"eval\"][task_id], key=lambda x: int(x[\"solution_id\"]))\n",
    "if work_dir.endswith(\"humaneval\"):\n",
    "    problems = get_human_eval_plus()\n",
    "else:\n",
    "    problems = get_mbpp_plus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79000it [00:11, 6699.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/mbpp/code-llama-13b-instruct_temp_1.6_debug3_sd-ut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(join(work_dir, model_name + debug_info), exist_ok=True)\n",
    "count_diff = 0\n",
    "for sample in tqdm(load_solutions(join(work_dir, gen_dir))):\n",
    "    task_id = sample[\"task_id\"]\n",
    "    p_name = task_id.replace(\"/\", \"_\")\n",
    "    os.makedirs(join(work_dir, model_name + debug_info, p_name), exist_ok=True)\n",
    "    solution_id = int(sample[\"solution_id\"])\n",
    "    assert eval_results[\"eval\"][task_id][solution_id][\"solution_id\"] == str(solution_id)\n",
    "    assert \"solution\" in sample\n",
    "    solution = (\n",
    "        sample[\"solution\"]\n",
    "        if \"solution\" in sample\n",
    "        else problems[task_id][\"prompt\"] + sample[\"completion\"]\n",
    "    )\n",
    "    \n",
    "    ### now we process the solution ###\n",
    "    #assert \"correct\" in solution or \"incorrect\" in solution, f\"{solution_id} {task_id} {solution}\"\n",
    "    if \"### fixed code\" in solution:\n",
    "        count_diff += 1\n",
    "        try:\n",
    "            assert \"```python\\n\" in solution, f\"{solution_id} {task_id} {solution}\"\n",
    "            if \"incorrect\" not in solution:\n",
    "                #print(solution_id, task_id)\n",
    "                #print(solution)\n",
    "                pass\n",
    "            new_solution = solution.split(\"```python\\n\")[1].strip()\n",
    "        except:\n",
    "            new_solution = \"\"\n",
    "        try:\n",
    "            assert \"```\" not in new_solution\n",
    "        except:\n",
    "            #print(task_id, solution_id)\n",
    "            #print(new_solution)\n",
    "            new_solution = solution.split(\"```\")[0].strip()\n",
    "    else:\n",
    "        try:\n",
    "            assert \"correct\" in solution\n",
    "        except:\n",
    "            #print(solution_id, task_id)\n",
    "            #print(solution)\n",
    "            pass\n",
    "        new_solution = eval_results[\"eval\"][task_id][solution_id][\"solution\"]\n",
    "    ### end of processing ###\n",
    "    \n",
    "    with open(join(work_dir, model_name + debug_info, p_name, f\"{solution_id}.py\"),\"w\",encoding=\"utf-8\",) as f:\n",
    "        f.write(new_solution)\n",
    "\n",
    "print(\"Saved to\", join(work_dir, model_name + debug_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32800it [00:01, 18122.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/humaneval/code-llama-7b-instruct_temp_1.6_debug2_sd-ut/continue_debug.json\n"
     ]
    }
   ],
   "source": [
    "continue_debug = {}\n",
    "for sample in tqdm(load_solutions(join(work_dir, gen_dir))):\n",
    "    task_id = sample[\"task_id\"]\n",
    "    if task_id not in continue_debug:\n",
    "        continue_debug[task_id] = {}\n",
    "    p_name = task_id.replace(\"/\", \"_\")\n",
    "    solution_id = int(sample[\"solution_id\"])\n",
    "    assert eval_results[\"eval\"][task_id][solution_id][\"solution_id\"] == str(solution_id)\n",
    "    assert \"solution\" in sample\n",
    "    solution = (\n",
    "        sample[\"solution\"]\n",
    "        if \"solution\" in sample\n",
    "        else problems[task_id][\"prompt\"] + sample[\"completion\"]\n",
    "    )\n",
    "    \n",
    "    ### now we process the solution ###\n",
    "    #assert \"correct\" in solution or \"incorrect\" in solution, f\"{solution_id} {task_id} {solution}\"\n",
    "    if \"### fixed code\" in solution:\n",
    "        continue_debug[task_id][solution_id] = True\n",
    "    else:\n",
    "        continue_debug[task_id][solution_id] = False\n",
    "    ### end of processing ###\n",
    "for task_id in continue_debug:\n",
    "    #sort by solution_id\n",
    "    continue_debug[task_id] = {str(k): v for k, v in sorted(continue_debug[task_id].items(), key=lambda x: int(x[0]))}\n",
    "\n",
    "#save continue_debug to json\n",
    "with open(join(work_dir, gen_dir.replace(\"_b4process\", \"\"), \"continue_debug.json\"), \"w\") as f:\n",
    "    print(\"Saved to\", join(work_dir, gen_dir.replace(\"_b4process\", \"\"), \"continue_debug.json\"))\n",
    "    json.dump(continue_debug, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20776"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evalplus.data import (\n",
    "    get_human_eval_plus,\n",
    "    get_human_eval_plus_hash,\n",
    "    get_mbpp_plus,\n",
    "    get_mbpp_plus_hash,\n",
    "    load_solutions,\n",
    ")\n",
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "work_dir = \"/mnt/scratch-artemis/haausing/code_reranking/evalplus_outputs/humaneval\"\n",
    "#model_name = \"deepseek-coder-7b-instruct-v1.5_temp_1.2\"\n",
    "model_name = \"code-llama-7b-instruct_temp_1.6\"\n",
    "debug_info = \"_debug1_sd-ut\"\n",
    "gen_dir = model_name + debug_info + \"_b4process\"\n",
    "original_dir = model_name + \"_debug1_sd-ut\"\n",
    "# read continue_debug.json\n",
    "with open(join(work_dir, original_dir, \"continue_debug.json\"), \"r\") as f:\n",
    "    continue_debug = json.load(f)\n",
    "\n",
    "count_continues = 0\n",
    "for task_id in continue_debug:\n",
    "    count_continues += sum(continue_debug[task_id].values())\n",
    "count_continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyminifier.minification import remove_comments_and_docstrings, remove_blank_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(code):\n",
    "    code = remove_comments_and_docstrings(code)\n",
    "    code = remove_blank_lines(code)\n",
    "    return code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evalplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
